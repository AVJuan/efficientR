---
knit: "bookdown::preview_chapter"
---
  
# Efficient data carpentry

In this chapter we will cover the following topics:


 - Importing data. This can depend on external packages and represent a time-consuming and computational bottle-neck that prevents progress.
 
 - Tidying data. This critical stage results in datasets that are convenient for analysis and processing, with implications for the efficiency of all subsequent stages [@Wickham_2014].
 
 - Data processing. This stage involves manipulating data to help answer hypotheses and draw conclusions. The focus of this section is on **dplyr** and **data.table**, which make data analysis code fast to type and fast to run.## Tidying data with tidyr

A key skill in data analysis is understanding the structure of datasets and being able to 'reshape' them. This is important from a workflow efficiency perspective: more than half of a data analyst's time can be spent re-formatting datasets [@Wickham_2014]. Converting data into a 'tidy' form is also advantageous from a computational efficiency perspective: it is usually faster to run analysis and plotting commands on a few large vectors than many short vectors.
This is illustrated by Tables \@ref(tab:tpew) and \@ref(tab:tpewt), provided by @Wickham_2014.

```{r, echo=FALSE, eval=FALSE}
# Download data from its original source - an academic paper
downloader::download("http://www.jstatsoft.org/v59/i10/supp/4", destfile = "v59i10-data.zip")
# The source code associated with the paper
downloader::download("http://www.jstatsoft.org/v59/i10/supp/3", destfile = "data/reshape/v59i10.R")
# After running the R script...
dir.create("data/reshape")
unzip("v59i10-data.zip", exdir = "data/reshape/")
# write.csv(raw, "data/reshape-pew.csv")
```

These tables may look different, but they contain precisely the same information.
Column names in the 'flat' form in Table \@ref(tab:tpew) became a new variable in the 'long' form in Table \@ref(tab:tpewt).
According to the concept of 'tidy data', the long form is correct.
Note that 'correct' here is used in the context of data analysis and graphical visualisation.
For tabular presentation the 'wide' or 'untidy' form may be better.

Tidy data has the following characteristics [@Wickham_2014]:

1. Each variable forms a column.
2. Each observation forms a row.
3. Each type of observational unit forms a table.

Because there is only one observational unit in the example (religions), it can be described in a single table.
Large and complex datasets are usually represented by multiple tables, with unique identifiers or 'keys' to join them together [@Codd1979]. 

Two common operations facilitated by **tidyr** are *gathering* and *splitting* columns.

- Gathering: this means making 'wide' tables 'long' by converting column names to a new variable. This is done is done with the function
`gather` (the inverse of which is `spread`) , as illustrated in Table \@ref(tab:tpew) and Table \@ref(tab:tpewt) and in the code block below:


```{r}
library("tidyr")
raw = read_csv("data/pew.csv") # read in the 'wide' dataset
dim(raw)
rawt = gather(raw, Income, Count, -religion)
dim(rawt)
rawt[1:3,]
```

```{block, gather-note, type='rmdtip'}
Note
that the dimensions of the data change from having 10 observations across 18 columns to 162 rows in only 3 columns.
Note that when we print the object `rawt[1:3,]`, the class of each variable is given
(`chr`, `fctr`, `int` refer to character, factor and integer classes, respectively).
This is because `read_csv` uses the `tbl` class from the **dplyr** package (described below).
```

```{r tpew, echo=FALSE}
# generate pew dataset
raw = read_csv("data/reshape-pew.csv")
raw = raw[-c(1,ncol(raw))] # remove excess cols
names(raw) = c("religion", "<$10k", "$10--20k", "$20--30k", "$30--40k",
               "$40--50k", "$50--75k", "$75--100k", "$100--150k", ">150k")
# write_csv(raw, "data/pew.csv")
knitr::kable(raw[1:3,1:4], caption = "First 6 rows of the aggregated 'pew' dataset from Wickham (2014a) in an 'untidy' form.")
```

```{r tpewt, echo=FALSE}
library("tidyr")
rawt = gather(raw, Income, Count, -religion)
rawt$Count = as.character(rawt$Count)
rawt$Income = as.character(rawt$Income)
rawtp = rawt[c(1:3, nrow(rawt)),]

insertRow = function(existingDF, newrow, r) {
  existingDF[seq(r+1,nrow(existingDF)+1),] <- existingDF[seq(r,nrow(existingDF)),]
  existingDF[r,] <- newrow
  existingDF
}

rawtp = insertRow(existingDF = rawtp, newrow = rep("...", 3), r = 4)
knitr::kable(rawtp, caption = "First 3 and last rows of the 'tidied' Pew dataset.")
```


- Splitting: this means taking a variable that is really two variables combined and creating two separate columns from it. A classic example is age-sex variables (e.g. `m0-10` and `f0-15` to represent males and females in the 0 to 10 age band). Splitting such variables can be done with `separate`, as illustrated in Table \@ref(tab:to-separate) and \@ref(tab:separated).

```{r}
agesex = c("m0-10", "f0-10") # create compound variable
n = c(3, 5) # create a value for each observation
df = data.frame(agesex, n) # create a data frame
separate(df, agesex, c("sex", "age"), 1)
```

```{r to-separate, echo=FALSE,}
knitr::kable(df, caption = "Joined age and sex variables in one column")
```

```{r separated, echo=FALSE}
knitr::kable(separate(df, agesex, c("sex", "age"), 1),
             caption = "Age and sex variables separated by the funtion `separate`.")
```

There are other tidying operations that **tidyr** can perform, as described in the package's vignette (`vignette("tidy-data")`).
Data manipulation is a large topic with major potential implications for efficiency, and there is an entire book on the subject [@Spector_2008].

## Data processing with dplyr {#dplyr}

Tidy data is easier and often faster to process than messy data. As with many aspects of R programming there are many ways to process a dataset, some more efficient than others. Following our own advice, we have selected a package for data processing early on (see Section \@ref(pkgs)): **dplyr**. This package, which rougly means 'data pliers' or 'plyr' (another R package) for large datasets, has a number of advantages compared with base R and **data.table** approaches to data processing:

- **dplyr** is fast to run and intuitive to type
- **dplyr** works well with tidy data, as described above
- **dplyr** works well with databases, providing efficiency gains on large datasets

We will illustrate the functioning of **dplyr** with reference to a dataset on economic equality provided by the World Bank. This is loaded in the following code block:

```{r, message=FALSE, results='hide'}
library("readr")
fname = system.file("extdata/world-bank-ineq.csv", package = "efficient")
idata = read_csv(fname)
idata # print the dataset 
```

**dplyr** is much faster than base implementations of various
operations, but it has the potential to be even faster, as
*parallelisation* is
[planned](https://github.com/hadley/dplyr/issues/145) and the [multidplyr](https://github.com/hadley/multidplyr) package, a parallel backend for **dplyr**, is under development.

You should not be expecting to learn the **dplyr** package in one sitting:
the package is large and can be seen as
a language in its own right. Following the 'walk before you run' principle,
we'll start simple, by filtering and aggregating rows, building on the previous section on tidying data.

### Renaming columns

Renaming data columns is a common task that can make writing code faster by using short, intuitive names. The **dplyr** function `rename()` makes this easy.

```{block, varname-block, style='rmdtip'}
Note
in this code block the variable name is surrounded by back-quotes (`).
This allows R to refer to column names that are non-standard.
Note also the syntax:
`rename` takes the `data.frame` as the first object and then creates new variables by specifying `new_variable_name = original_name`.
```

```{r, message=FALSE}
library("dplyr")
idata = rename(idata, Country = `Country Name`)
```

To rename multiple columns the variable names are simply separated by commas. The base R and **dplyr** way of doing this is illustrated for clarity.

```{r}
# The dplyr way (rename two variables)
idata = rename(idata,
 top10 = `Income share held by highest 10% [SI.DST.10TH.10]`,
 bot10 = `Income share held by lowest 10% [SI.DST.FRST.10]`)
# The base R way (rename five variables)
names(idata)[5:9] = c("top10", "bot10", "gini", "b40_cons", "gdp_percap")
```

Now we have usefully renamed the object we save the result for future reference:

```{r, eval=FALSE}
saveRDS(idata, "data/idata-renamed.Rds")
```

### Changing column classes

The *class* of R objects is critical to performance.
If a class is incorrectly specified (e.g. if numbers are treated as factors or characters) this will lead to incorrect results. The class of all columns in a `data.frame` can be queried using the function `sapply()`, as illustrated below, with the inequality data loaded previously.

```{r}
idata = readRDS("data/idata-renamed.Rds")
sapply(idata, class)
```

This shows that although we loaded the data correctly all columns are seen by R as characters. This means we cannot perform numerical calculations on the dataset: `mean(idata$gini)` fails.

Visual inspection of the data (e.g. via `View(idata)`) clearly shows that all columns except for 1 to 4 ("Country", "Country Code", "Year" and "Year Code") should be numeric. We can re-assign the classes of the numeric variables one-by one:

```{r}
idata$gini = as.numeric(idata$gini)
mean(idata$gini, na.rm = TRUE) # now the mean is calculated
```

However, the purpose of programming languages is to *automate* tasks and reduce typing. The following code chunk re-classifies all of the numeric variables using `data.matrix()`, which converts a `data.frame` to a numeric `matrix`:

```{r, warning=FALSE}
id = 5:9 # column ids to change
idata[id] = data.matrix(idata[id])
sapply(idata, class)
```

As is so often the case with R, there are many ways to solve the problem. Below is a one-liner using `unlist()` which converts list objects into vectors:

```{r, warning=FALSE}
idata[id] = as.numeric(unlist(idata[id]))
```

*Another* one-liner to acheive the same result uses **dplyr**'s `mutate_each` function: 

```{r, warning=FALSE}
idata = mutate_each(idata, funs(as.numeric), id)
```

As with other operations there are other ways of achieving the same result in R, including the use of loops via `apply()` and `for()`. These are shown in the chapter's [source code](https://github.com/csgillespie/efficientR).

```{r, echo=FALSE, warning=FALSE}
# Idea: these 4 methods could be benchmarked
# An alternative method for changing the column class
idata[id] = apply(idata[id], 2, as.numeric)

# Another alternative method using a for loop
idata.df = as.data.frame(idata)
for(i in id){
  idata.df[,i] = as.numeric(idata.df[,i])
}
```

### Filtering rows

The standard way to subset data by rows in R is with square brackets, for example:

```{r}
aus1 = idata[idata$Country == "Australia",]
```

**dplyr** offers an alternative and more flexible way of filtering data, using `filter()`.

```{r}
aus2 = filter(idata, Country == "Australia")
```

In addition to being more flexible (see `?filter`), `filter` is slightly faster than base R's notation.^[Note that `filter` is also the name of a function used in the base **stats** library. Usually packages avoid using names already taken in base R but this is an exception.]
Note that **dplyr** does not use the `$` symbol: it knows that that `Country` is a variable of `idata`:
the first argument of **dplyr** functions usually a `data.frame`, and
subsequent in this context variable names can be treated as vector objects.^[Note that this syntax is a defining feature of **dplyr**
and many of its functions work in the same way.
Later we'll learn how this syntax can be used alongside the `%>%` 'pipe' command to write clear data manipulation commands.
] 

There are **dplyr** equivalents of many base R functions but these usually work slightly differently. The **dplyr** equivalent of `aggregate`, for example is to use the grouping function `group_by` in combination with the general purpose function `summarise` (not to be confused with `summary` in base R), as we shall see in Section \@ref(data-aggregation). For consistency, however, we next look at filtering columns.

### Filtering columns

Large datasets often contain much worthless or blank information. This consumes RAM and reduces computational efficiency. Being able to focus quickly only on the variables of interest becomes especially important when handling large datasets.

Imagine that we have a text file called `miniaa` which is large enough to consume most of your computer's RAM. We can load it with the following command:

```{r}
fname = system.file("extdata/miniaa", package = "efficient")
df = read.csv(fname) # load imaginary large data
dim(df)
```

Note that the data frame has 329 columns, and imagine it has millions of rows, instead of 9. That's a lot of variables. Do we need them all? It's worth taking a glimpse at this dataset to find out:

```{r, eval=FALSE}
glimpse(df)
#> $ NPI                   (int) 1679576722, ...
#> $ Entity Type Code      (int) 1, 1, 2,    ...
#> $ Replacement NPI       (lgl) NA, NA, NA, ...
#> ...
```

Looking at the output, it becomes clear that the majority of the variables only contain `NA`. To clean the giant dataset, removing the empty columns, we need to identify which variables these are.

```{r, echo=FALSE}
# TODO: demonstrate dplyr::select
```

```{r}
# Identify the variable which are all NA
all_na = sapply(df, function(x) all(is.na(x)))
summary(all_na) # summary of the results
df1 = df[!all_na] # subset the dataframe
```

The new `df` object has fewer than a third of the original columns. Another way to save storage space, beyond removing the superfluous columns, is to save the dataset in R's binary data format:

```{r}
saveRDS(df1, "data/miniaa.Rds")
```


#### Exercises

1. How much space was saved by reducing the number of columns? (Hint: use `object.size()`.)

2. How many times smaller is the .Rds file saved above compared with the .csv file? (Hint: use `file.size()`.)

```{r, include=FALSE}
object.size(df1) / object.size(df)
file.size(fname) / file.size("data/miniaa.Rds")
```

### Data aggregation

Data aggregation is the process of creating summaries of data based on a grouping variable. The end result usually has the same number of rows as there are groups. Because aggregation is a way of condensing datasets it can be a very useful technique for making sense of large datasets. The following code finds the number of unique countries (country being the grouping variable) from the 'GHG' dataset stored in the **efficient** package.

```{block, ghg-data, type='rmdnote'}
The GHG dataset used in the subsequent code reports the amount of greenhouse gas emissions emitted by country and by year for the major economic sectors. It was provided by the World Resources Institute and is available in raw form from their website: [wri.org/resources/data-sets/](http://www.wri.org/resources/data-sets/cait-country-greenhouse-gas-emissions-data).
```

```{r, warning=FALSE}
fname = system.file("extdata/ghg-ems.csv", package = "efficient")
df = read.csv(fname)
names(df)
nrow(df)
length(unique(df$Country))
```

Note that while there are almost 8000 rows, there are less than 200 countries. Referring back to Section \@ref(renaming-columns), the next stage should be to rename the columns so they are more convenient to work with. Having checked the verbose column names, this can be done in base R using the following command:

```{r}
names(df)[4:8] = c("ECO2", "MCO2", "TCO2", "OCO2", "FCO2")
```

After the variable names have been updated, we can aggregate.^[Note the first argument in the function is the vector we're aiming to aggregate and the second is the grouping variable (in this case Countries).
A quirk of R is that the grouping variable must be supplied as a list.
Next we'll see a way of writing this that is neater.]

```{r}
e_ems = aggregate(df$ECO2, list(df$Country), mean, na.rm  = TRUE)
nrow(e_ems)
```

Note that the resulting data frame now has the same number of rows as there are countries:
the aggregation has successfully reduced the number of rows we need to deal with.
Now it is easier to find out per-country statistics, such as the three lowest emitters from electricity production:

```{r}
head(e_ems[order(e_ems$x),], 3)
```

Another way to specify the `by` argument is with the tilde (`~`).
The following command creates the same object as `e_ems`, but with less typing.

```{r}
e_ems = aggregate(ECO2 ~ Country, df, mean, na.rm  = TRUE)
```

To aggregate the dataset using **dplyr** package one would divide the task in two: to *group* the dataset first and then to summarise, as illustrated below:

```{r, message=FALSE}
library("dplyr")
group_by(df, Country) %>%
  summarise(mean_eco2 = mean(ECO2, na.rm  = TRUE))
```


```{r}
countries = group_by(idata, Country)
summarise(countries, gini = mean(gini, na.rm  = TRUE))
```

Note that `summarise` is highly versatile, and can be used to return a customised range of summary statistics:

```{r tidy=FALSE}
summarise(countries,
  # number of rows per country
  obs = n(), 
  med_t10 = median(top10, na.rm  = TRUE),
  # standard deviation
  sdev = sd(gini, na.rm  = TRUE), 
  # number with gini > 30
  n30 = sum(gini > 30, na.rm  = TRUE), 
  sdn30 = sd(gini[ gini > 30 ], na.rm  = TRUE),
  # range
  dif = max(gini, na.rm  = TRUE) - min(gini, na.rm  = TRUE)
  )
```

To showcase the power of `summarise` used on
a `grouped_df`, the
above code reports a wide range of customised
summary statistics
*per country*: 

- the number of rows in each country group
- standard deviation of gini indices
- median proportion of income earned by the top 10%
- the number of years in which the gini index was greater than 30
- the standard deviation of gini index values over 30
- the range of gini index values reported for each country.

#### Exercises

1. Referring back to Section \@ref(renaming-columns), rename the variables 4 to 8 using the **dplyr** function `rename`. Follow the pattern `ECO2`, `MCO2` etc.

```{r, echo=FALSE, eval=FALSE}
# Using dplyr::rename
df = rename(df,
             ECO2 = Electricity.Heat..CO2...MtCO2.,
             MCO2 = Manufacturing.Construction..CO2...MtCO2.,
             TCO2 = Transportation..CO2...MtCO2.,
             FCO2 = Fugitive.Emissions..CO2...MtCO2.)
```

2. Explore **dplyr**'s documentation, starting with the introductory vignette, accessed by entering [`vignette("introduction")`](https://cran.rstudio.com/web/packages/dplyr/vignettes/introduction.html).

3. Test additional **dplyr** 'verbs' on the `idata` dataset. (More vignette names can be discovered by typing `vignette(package = "dplyr")`.)

### Chaining operations

Another interesting feature of **dplyr** is its ability
to chain operations together. This overcomes one of the
aesthetic issues with R code: you can end end-up with
very long commands with many functions nested inside each
other to answer relatively simple questions.

> What were, on average, the 5 most unequal
years for countries containing the letter g?

Here's how chains work to organise the analysis in a
logical step-by-step manner:

```{r tidy=FALSE}
idata %>% 
  filter(grepl("g", Country)) %>%
  group_by(Year) %>%
  summarise(gini = mean(gini, na.rm  = TRUE)) %>%
  arrange(desc(gini)) %>%
  top_n(n = 5)
```

The above function consists of 6 stages, each of which
corresponds to a new line and **dplyr** function:

1. Filter-out the countries we're interested in (any selection criteria could be used in place of `grepl("g", Country)`).
2. Group the output by year.
3. Summarise, for each year, the mean gini index.
4. Arrange the results by average gini index
5. Select only the top 5 most unequal years.

To see why this method is preferable to the nested
function approach, take a look at the latter.
Even after indenting properly it looks terrible
and is almost impossible to understand!

```{r, eval=FALSE, tidy=FALSE}
top_n(
  arrange(
    summarise(
      group_by(
        filter(idata, grepl("g", Country)),
        Year),
      gini = mean(gini, na.rm  = TRUE)),
    desc(gini)),
  n = 5)
```

```{r, echo=FALSE}
# # Removed - should illustrate how to do it in base R if mentioned:
# Of course, you *could* write code in base R to
# undertake the above analysis but for many
# people the **dplyr** approach is the most agreeable to write.
```

This section has provided only a taster of what is possible **dplyr** and why it makes sense from code writing and computational efficiency perspectives. For a more detailed account of data processing with R using this approach we recommend *R for Data Science* [@grolemund_r_2016].

## Data processing with data.table

**data.table** is a mature package for fast data processing that presents an alternative to **dplyr**. There is some controversy about which is more appropriate for different
tasks^[One
[question](http://stackoverflow.com/questions/21435339) on the stackoverflow website titled 'data.table vs dplyr' illustrates this controversey and delves into the philosophy underlying each approach.
]
so it should be stated at the outset that **dplyr** and **data.table** are not mutually exclusive competitors. Both are excellent packages and the important thing from an efficiency perspective is that they can help speed up data processing tasks.

```{r, echo=FALSE}
# This section provides a few examples to illustrate how **data.table** differs and (at the risk of inflaming the debate further) some benchmarks to explore which is more efficient. As emphasised throughout the book, efficient code writing is often more important than efficient execution on many everyday tasks so to some extent it's a matter of preference.
```

The foundational object class of **data.table** is the `data.table`. Like **dplyr**'s `tbl_df`, **data.table**'s `data.table` objects behave in the same was as the base `data.frame` class. However the **data.table** paradigm has some unique features that make it highly computationally efficient for many common tasks in data analysis. Building on subsetting methods using `[` and `filter()` presented in Section \@ref(filtering-columns), we'll see **data.tables**'s unique approach to subsetting. Like base R **data.table** uses square brackets but you do not need to refer to the object name inside the brackets:

```{r}
library("data.table")
idata = readRDS("data/idata-renamed.Rds")
idata_dt = data.table(idata) # convert to data.table class
aus3a = idata_dt[Country == "Australia"]
```

To boost performance, one can set 'keys'. These are
'[supercharged rownames](https://cran.r-project.org/web/packages/data.table/vignettes/datatable-keys-fast-subset.html)'
which order the table based on one or more variables. This allows a *binary search* algorithm to subset the rows of interest, which is much, much faster than the *vector scan* approach used in base R (see [`vignette("datatable-keys-fast-subset")`](https://cran.r-project.org/web/packages/data.table/vignettes/datatable-keys-fast-subset.html)). **data.table** uses the key values for subsetting by default so the variable does not need to be mentioned again. Instead, using keys, the search criteria is provided as a list (invoked below with the concise `.()` syntax below).

```{r}
setkey(idata_dt, Country)
aus3b = idata_dt[.("Australia")]
```

The result is the same, so why add the extra stage of setting the key? The reason is that this one-off sorting operation can lead to substantial performance gains in situations where repeatedly subsetting rows on large datasets consumes a large proportion of computational time in your workflow. This is illustrated in Figure \@ref(fig:dtplot), which compares 4 methods of subsetting incrementally larger versions of the `idata` dataset. 

```{r, eval=FALSE, echo=FALSE}
res = NULL
for(i in seq(10, 1000, length.out = 10)){
  idata_big = do.call("rbind", replicate(i, idata, simplify = FALSE))
  idata_dt = as.data.table(idata_big)
  idata_key = copy(idata_dt) # copy the object
  setkey(idata_key, "Country")
  mb = microbenchmark(
    times = 1,
    base_sqrbrkt = idata_big[idata_big$Country == "Australia", ],
    dplyr_filter = filter(idata_big, Country == "Australia"),
    dt_standard = idata_dt[Country == "Australia"],
    dt_key = idata_key[list("Australia"), ]
  )
  tab = tapply(mb$time / 1000, mb$expr, mean)
  res_tmp = data.frame(
    exp = names(tab),
    time = as.vector(tab),
    rows = nrow(idata_big),
    MB = as.numeric(object.size(idata_big) / 1000000)
  )
  res_tmp$Time = res_tmp$time / min(res_tmp$time)
  res = rbind(res, res_tmp)
}
saveRDS(res, "data/res-datatable.Rds")
```

```{r dtplot, fig.cap="Benchmark illustrating the performance gains to be expected for different dataset sizes.", echo=FALSE, fig.width=6, fig.height=4}
local(source("code/04-project-planning_f4.R", local=TRUE))
```

Figure \@ref(fig:dtplot) demonstrates that **data.table** is *much faster* than base R and **dplyr** at subsetting. As with using external packages to read in data (see Section \@ref(fread)), the relative benefits of **data.table** improve with dataset size, approaching a ~70 fold improvement on base R and a ~50 fold improvement on **dplyr** as the dataset size reaches half a Gigabyte. Interestingly, even the 'non key' implementation of **data.table** subset method is faster than the alternatives: this is because **data.table** creates a key internally by default before subsetting. The process of creating the key accounts for the ~10 fold speed-up in cases where the key has been pre-generated.

This section has introduced **data.table** as a complimentary approach to base and **dplyr** methods for data processing and illustrated the performance gains of using *keys* for subsetting tables. **data.table** is a mature and powerful package which uses clever computational principles implemented in C to provide efficient methods for a number of other operations for data analysis. These include highly efficient data reshaping, dataset merging (also known as joining, as with `left_join` in **dplyr**) and grouping. These are explained in the vignettes [`datatable-intro`](https://cran.r-project.org/web/packages/data.table/vignettes/datatable-intro.pdf) and [`datatable-reshape`](https://cran.r-project.org/web/packages/data.table/vignettes/datatable-reshape.html). The [`datatable-reference-semantics`](https://cran.r-project.org/web/packages/data.table/vignettes/datatable-reference-semantics.html) vignette explains **data.table**'s unique syntax.
