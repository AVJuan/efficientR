---
knit: "bookdown::preview_chapter"
---
  
# Efficient Performance

Hints and tips for performance


## The integer data type

Numbers in R are usually stored in [double-precision floating-point format](https://goo.gl/ZA5R8a) - see @Braun2007 and @Goldberg1991. The term 'double' refers to the fact that on $32$ bit systems (for which the format was developed) two memory locations are used to store a single number. Each double-precision number occupies $8$ bytes and is accurate to around $17$ decimal places (R does not print all of these, as you will see by typing `pi`). 

```{block type="rmdnote"}
When comparing floating point numbers, we should be particularly careful, since `y = sqrt(2)*sqrt(2)` 
is not exactly $2$, instead it's __almost__ $2$. Using `sprintf("%.16f", y)` will
give you the true value of `y` (to 16 decimal places).
```

There is also another data type, called an integer. Integers primarily exist to be passed to C or Fortran code. Typically we don't worry about creating integers. However they are occasionally used to optimise sub-setting operations. When we subset a data frame or matrix, we are interacting with C code. For example, if we look at the arguments for the `head` function

```{r}
args(head.matrix)
```

The default argument is `6L` (the `L`, is short for Literal and is used to create an integer). Since this function is being called by almost everyone that uses R, this low level optimisation is useful. To illustrate the speed increase, suppose we are selecting the first $100$ rows from a data frame (`clock_speed`, from the **efficient** package). The speed increase is illustrated below, using the **microbenchmark** package:

```{r, matrix_timing, eval=FALSE}
s_int = 1:100; s = seq(1, 100, 1.0)
microbenchmark(clock_speed[s_int, 2L], clock_speed[s, 2.0], times=1000000L)
```

```
## Unit: microseconds
## expr   min    lq  mean median    uq   max neval cld
## clock_speed[s_int, 2L] 11.79 13.43 15.30  13.81 14.22 87979 1e+06  a 
## clock_speed[s, 2] 12.79 14.37 16.04  14.76 15.18 21964 1e+06   b
```

The above result shows that using integers is slightly faster, but probably not worth worrying about.

A related, unmentioned memory saving idea is to replace `logical` vectors with vectors from the `bit` package which take up just over 16th of the space (but you can't use NAs).


integer` vectors take up just over half the memory of `numeric` vectors, so if memory is a problem, use those in preference.

## Matrices

A matrix is similar to a data frame: it is a two dimensional object and sub-setting and other functions work in the same way. However all matrix columns must have the same type. 
Matrices tend to be used during statistical calculations. 
Linear regression using `lm()`, for example, internally converts the data to a matrix before calculating the results; any characters are thus recoded as numeric dummy variables.

Matrices are generally faster than data frames. 
The datasets `ex_mat` and `ex_df` from the **efficient** package each have $1000$ rows and $100$ columns. 
They contain the same random numbers. However, selecting rows from a data frame is around $150$ times slower than a matrix. 

```{r mat_vs_df, echo=2:4, cache=TRUE}
library("rbenchmark")
data(ex_mat, ex_df, package="efficient")
benchmark(replications=10000, 
          ex_mat[1,], ex_df[1,], 
          columns=c("test", "elapsed", "relative"))
```

Use `data.matrix` to convert

### Efficient data structures

Even when our data set is small, the analysis can generate large objects. For example suppose we want to perform standard cluster analysis. Using the built-in data set `USAarrests`, we calculate a distance matrix:

```{r}
dist_usa = dist(USArrests)
```

The resulting object `dist_usa` measures the similarity between two states with respect to the input data. Since there are $50$ states in the `USAarrests` data set, this results in a matrix with $50$ columns and $50$ rows. Intuitively, since the matrix `dist_usa` is symmetric around the diagonal, it makes sense to exploit this characteristic for efficiency, allowing storage to be halved. If we examine the object `dist_usa`, with `str(dist_usa)`, it becomes apparent that the data is efficiently stored as a vector with some attributes.

Another efficient data structure is a sparse matrix. This is simply a matrix in where most of the elements are zero. Conversely, if most elements are non-zero, the matrix is considered dense. The proportion of non-zero elements is called the sparsity. Large sparse matrices often crop up when performing numerical calculations. Typically, our data isn't sparse but the resulting data structures we create may be sparse. There are a number of techniques/methods used to store sparse matrices. Methods for creating sparse matrices can be found in the **Matrix** package. For this `dist` object, since the structure is regular.

## ifelse

```
Further note that if(test) yes else no is much more efficient and often much preferable to ifelse(test, yes, no) whenever test is a simple true/false result, i.e., when length(test) == 1.
```

## Sorting and ordering

### method = c("shell", "quick", "radix")
### partial
### Don't use rev  sort(x, decreasing = TRUE). from ?rev

## which.min and match

## Converting factors to numerics

```
## what you typically meant and want:
as.numeric(as.character(f))
## the same, considerably (for long factors) more efficient:
as.numeric(levels(f))[f]
```
```
?paste
paste0(..., collapse) is equivalent to paste(..., sep = "", collapse), slightly more efficiently.
```

## nrow and NRWO

## && and &

## on.exit and invisible


## The byte compiler

<!-- What's "pre-compilation" and what's "byte-code"?  Your explanation comes a couple of paragraphs after you use the terminology, so this section needs reordering a little. -->

## ::



The ** compiler** package, written by R Core member Luke Tierney has been part of R since version 2.13.0. Since R 2.14.0, all of the standard functions and packages in base R are pre-compiled into byte-code. This is illustrated by the base function `mean`:

```{r}
mean
```

The third line contains the `bytecode` of the function. This means that the **compiler** package has translated the R function into another language that can be interpreted by a very fast interpreter.  

The **compiler** package allows R functions to be compiled, resulting in a byte code version that may run faster^[The authors have yet to find a situation where byte compiled code runs significantly slower.]. The compilation process eliminates a number of costly operations the interpreter has to perform, such as variable lookup. Amazingly the compiler package is almost entirely pure R, with just a few C support routines. 

### Example: the mean function

The **compiler** package comes with R, so we just need to load the package in the usual way

```{r}
library("compiler")
```

Next we create an inefficient function for calculating the mean. This function takes in a vector, calculates the length and then updates the `m` variable.

```{r}
mean_r = function(x) {
  m = 0
  n = length(x)
  for(i in 1:n)
    m = m + x[i]/n
  m
}
```

This is clearly a bad function and we should just `mean` function, but it's a useful comparison. Compiling the function is straightforward

```{r}
cmp_mean_r = cmpfun(mean_r)
```

Then we use the `benchmark` function to compare the three variants


<!-- Make n bigger and just copy and paster output -->
```{r results="hide", cache=TRUE}
# Generate some data
x = rnorm(100)
benchmark(mean_r(x), cmp_mean_r(x), mean(x), 
          columns=c("test", "elapsed", "relative"),
          order="relative", replications=5000)
```

The compiled function is around seven times faster than the uncompiled function. Of course, the native `mean` function is faster, but the compiling does make a significant difference (figure \@ref(fig:6-4)).

```{r 6-4, echo=FALSE, fig.height=4, fig.width=4, fig.cap="Comparsion of mean functions.", eval=TRUE}
load(file="data/mean_comparison.RData")
dd = mean_comparison
library("ggplot2")
g = ggplot(dd, aes(p, relative)) + 
  geom_line(aes(colour=test), lwd=1) + 
  theme_bw() + 
  xlab("Sample size") + 
  ylab("Relative timings") + 
  scale_x_continuous(trans="log10", breaks=c(100, 1000, 10000), 
                     labels=c(expression(10^2), expression(10^3), expression(10^4))) + 
  scale_colour_manual(values=colours, guide=FALSE) +
  ylim(c(0, 200))
g + annotate("text", x=1000, y=90, label="Pure R", col=colours[3], size=3) +
  annotate("text", x=1000, y=20, label="Complied R", col=colours[1], size=3) + 
  annotate("text", x=8000, y=10, label="mean", col=colours[2], size=3)
```


### Compiling code

There are a number of ways to compile code. The easiest is to compile individual function using `cmpfun`, but this obviously doesn't scale. If you create a package, then you automatically compile the package on installation by adding
```
ByteCompile: true
```
to the `DESCRIPTION` file. Most R packages installed using `install.packages` are not compiled. We can enable (or force) packages to be compiled by starting R with the environment variable `R_COMPILE_PKGS` set to a positive integer value.

A final option to use just-in-time (JIT) compilation. The `enableJIT` function disables JIT compilation if the argument is `0`. Arguments `1`, `2`, or `3` implement different levels of optimisation. JIT can also be enabled by setting the environment variable `R_ENABLE_JIT`, to one of these values.


```{r eval=FALSE, echo=FALSE}
dd = NULL
for(i in seq(2, 4, length.out=12)) {
  x = rnorm(10^i)
  dd_tmp = rbenchmark::benchmark(my_mean(x), cmp_mean(x), mean(x), 
                                 columns=c("test", "elapsed", "relative"),
                                 order="relative", replications=5000)
  dd_tmp$i = i
  dd = rbind(dd, dd_tmp)
}
dd$p = 10^dd$i
dir.create("data", showWarnings = FALSE)
mean_comparison = dd
save(mean_comparison, file="data/mean_comparison.RData")
```

## Type consistency


That is, when you are programming, it is very helpful if the return value from a function always takes the same form.  For example `sapply` and `[.data.frame` aren't type consistent, which can cause problems.

```{r}
two_cols <- data.frame(x = 1:5, y = letters[1:5])
zero_cols <- data.frame()
sapply(two_cols, class)  # a character vector
sapply(zero_cols, class) # a list
two_cols[, 1:2]          # a data.frame
two_cols[, 1]            # an integer vector
```

`lapply` and `vapply` are type consistent.  Likewise `dplyr::select` and `dplyr:filter` are type consistent.  `[.data.table` can be type consistent if you are careful about inputs. The *purrr* package has some type consistent alternatives to base R functions.  For example, `map_dbl` etc. to replace `Map` and `flatten_df` etc. to replace `unlist`.














