---
knit: "bookdown::preview_chapter"
---

```{r echo=FALSE}
source("code/initialise.R")
```
# Efficient Performance

General Hints and tips for performance. 

In previous chapters we've focused on tools and set-up to increase efficiency. In this
chapter we are going to consider code optimisation. We will consider a variety of code
tweaks that can eke out an extra speed boost.

## The integer data type

Numbers in R are usually stored in [double-precision floating-point
format](https://goo.gl/ZA5R8a) - see @Braun2007 and @Goldberg1991. The term 'double'
refers to the fact that on $32$ bit systems (for which the format was developed) two
memory locations are used to store a single number. Each double-precision number is
accurate to around $17$ decimal places.

```{block type="rmdnote"}
When comparing floating point numbers we should be particularly careful, since `y = sqrt(2)*sqrt(2)` 
is not exactly $2$, instead it's __almost__ $2$. Using `sprintf("%.17f", y)` will
give you the true value of `y` (to 17 decimal places).
```

There is also another data type, called an integer. Integers primarily
exist to be passed to C or Fortran code. Typically we don't worry about creating
integers, however they are occasionally used to optimise sub-setting
operations. When we subset a data frame or matrix, we are interacting with C code.
For example, if we look at the arguments for the `head` function

```{r}
args(head.matrix)
```

```{block, type="rmdtip"}
Using the `:` operator automatically creates a vector of integers.
```

The default argument is `6L` (the `L`, is short for Literal and is used to create an
integer). Since this function is being called by almost everyone that uses R, this low
level optimisation is useful. To illustrate the speed increase, suppose we are
selecting the first $100$ rows from a data frame (`clock_speed`, from the
**efficient** package). The speed increase is illustrated below, using the
**microbenchmark** package:

```{r, matrix_timing, eval=FALSE}
s_int = 1:100; s_double = seq(1, 100, 1.0)
microbenchmark(clock_speed[s_int, 2L], clock_speed[s_double, 2.0], times=1000000)
```

```
## Unit: microseconds
## expr   min    lq  mean median    uq   max neval cld
## clock_speed[s_int, 2L] 11.79 13.43 15.30  13.81 14.22 87979 1e+06  a 
## clock_speed[s_double, 2] 12.79 14.37 16.04  14.76 15.18 21964 1e+06   b
```

The above result shows that using integers is slightly faster, but probably not worth
worrying about.

#### Exercises {-}

1. Another benefit of integers, is that they are slightly more memory efficient. Using
the `object_size` function from the **pryr** package, compare the size of a vector of
doubles and a vector of integers.

1. Examine the following function definitions to give you an idea of how integers are used.
  * `tail.matrix`
  * `lm`. 

1. How does the function `seq.int`, which was used in the `tail.matrix` function,
differ to the standard `seq` function?

```{block, type="rmdnote"}
A related memory saving idea is to replace `logical` vectors
with vectors from the **bit** package which take up just over 16th of the space
(but you can't use `NA`s).
```

## Matrices

A matrix is similar to a data frame: it is a two dimensional object and sub-setting
and other functions work in the expected way. However all matrix elements must have
the same type. Matrices tend to be used during statistical calculations. Calculating
the line of best fit using the `lm()` function, internally converts the data to a
matrix before calculating the results; any characters are thus recoded as numeric
dummy variables.

Matrices are generally faster than data frames. The datasets `ex_mat` and
`ex_df` from the **efficient** package each have $1000$ rows and $100$
columns. They contain the same random numbers. However selecting rows from
a data frame is around $150$ times slower than a matrix.

```{r mat_vs_df, cache=TRUE, results="hide"}
data(ex_mat, ex_df, package="efficient")
rbenchmark::benchmark(replications=10000, 
          ex_mat[1,], ex_df[1,], 
          columns=c("test", "elapsed", "relative"))
```

```{block, type="rmdtip"}
Use the `data.matrix` function to efficiently convert a data frame into a matrix.
```

### Efficient data structures

Even when our data set is small, an analysis can generate large objects. For example
suppose we want to perform standard cluster analysis. Using the built-in data set
`USAarrests`, we calculate a distance matrix:

```{r}
dist_usa = dist(USArrests)
```

The resulting object `dist_usa` measures the similarity between two states with
respect to the input data. Since there are $50$ states in the `USAarrests` data set,
this results in a matrix with $50$ columns and $50$ rows. Intuitively, since the
matrix `dist_usa` is symmetric around the diagonal, it makes sense to exploit this
characteristic for efficiency, allowing storage to be halved. If we examine the object
`dist_usa`, with `str(dist_usa)`, it becomes apparent that the data is efficiently
stored as a vector with some attributes.

Another efficient data structure is a sparse matrix. This is simply a matrix in where
most of the elements are zero. Conversely, if most elements are non-zero, the matrix
is considered dense. The proportion of non-zero elements is called the sparsity. Large
sparse matrices often crop up when performing numerical calculations. Typically, our
data isn't sparse but the resulting data structures we create may be sparse. There are
a number of techniques/methods used to store sparse matrices. Methods for creating
sparse matrices can be found in the **Matrix** package. For this `dist` object, since
the structure is regular.

<!-- (Don't like this title) -->

## Efficient base R 

In R, there are often more than one way to solve a problem. In this section we
highlight standard tricks or alternative methods that will improve performance.

### The `if` vs `ifelse` functions {-}

The `ifelse` function 
```{r eval=FALSE}
ifelse(test, yes, no)
```
is a vectorised version of the standard control-flow `if` function. The return value
of `ifelse` is filled with elements from the `yes` and `no` arguments that are
determined by whether the element of `test` is `TRUE` or `FALSE`.

If `test` has length 1, then the standard `if(test) yes else no` is more efficient.

### Sorting and ordering {-}

Sorting a vector is a relatively expensive computational operation. If you only sort a
vector once at the top of a script, then don't worry too much about this. However if
you sorting inside a loop, or in a shiny application, then it can be worthwhile
thinking about how to optimise this operation.

There are currently three sorting algorithms, `c("shell", "quick", "radix")` that can
be specified in the `sort` function; with `radix` being a new addition to R 3.3.
Typically the `radix` (the non-default option) is optimal for most situations.

Another useful trick is to partially order the results. For example, if you only want
to display the top ten results, then use the `partial` argument, i.e. `sort(x, partial=1:10)`

```{r, eval=FALSE, echo=FALSE}
x = matrix(rnorm(1e7), ncol=100)
system.time({
  for(i in 1:100)
    sort(x[,i], method="radix")
})
```

### Reversing elements {-}

The `rev` function provides a reversed version of its argument. If you wish to sort in
increasing order, you should use the more efficient `sort(x, decreasing=TRUE)` instead
of `rev(sort(x))`.

### Which indices are `TRUE` \ {-}

To determine which index of a vector or array are `TRUE`, we would typically use the
`which` function. If we want to find the index of just the minimum or maximum value,
i.e. `which(x == min(x))` then use the more efficient `which.min`/`which.max`
functions.

###  Converting factors to numerics {-}

A factor is just a vector of integers with associated levels. Occasionally, we want
to convert a factor into its numerical equivalent. The most efficient way of doing
this (especially for long factors) is

```{r, echo=2, results="hide"}
f = factor(10:100)
as.numeric(levels(f))[f]
```

where `f` is the factor.

### String concatenation {-}

To concatenate strings use the `paste` function
```{r, results="hide"}
paste("A", "B")
```
The separation character is specified via the `sep` argument. The function
`paste0(..., collapse)` is equivalent to `paste(..., sep = "", collapse)`, but is
slightly more efficient.

### Logical AND and OR {-}

The logical AND (`&`) and OR (`|`) operators are vectorised functions and are
typically used whenever we perform subsetting operations. For example

```{r, echo=-1}
x = c(0.2, 0.5, 0.7)
x < 0.4 | x > 0.6
```

When R executes the above comparison, it will **always** calculate `x > 0.6`
regardless of the value of `x < 0.4`. In contrast, the non-vectorised version, `&&`,
only executes the second component if needed. This is efficient and leads to more
neater code, e.g.

```{r}
## read.csv is only executed if the file exists
file.exists("data.csv") && read.csv("data.csv")
```

However care must be taken not to use `&&` or `||` on vectors since it will give the
incorrect answer
```{
x < 0.4 || x > 0.6
```
### Row and column operations


### Exit functions with care {-}

Occasionally in a function we make changes outside of the function that should be
reset after the function exits (either naturally or as the result of an error). The
`on.exit` function does this job with the minimum of fuss. For example, we could
rewrite the S3 image function as

```{r}
image.dist = function(x, ...) {
  op = par(mar=c(1, 1, 2, 1), mgp=c(0, 0, 0))
  on.exit(op)
  x_mat = as.matrix(x)
  image(x_mat, main=attr(x, "method"), ...)  
}
```

The `par` function changes graphical parameters, with `on.exit` ensuring these
parameters are reset to their previous value when the function ends. Using `on.exit`
with the **parallel** function `stopCluster` is also recommended.

### Invisible returns {-}

The `invisible` function allows you to return a temporarily invisible copy of an
object. This is particularly useful for functions that return values which can be
assigned, but are not printed when they are not assigned. For example, suppose we have
a function that plots the data and fits a straight line
```{r}
regression_plot = function(x, y, ...) {
  plot(x, y, ...)
  model = lm(y ~ x)
  abline(model)
  invisible(model)
}
```
When call the function, we plot a scatter graph with line of best fit; the output is
invisible. When we assign the function to an object, i.e. `out = regression_plot(x, y)`
the variable out contains the output of the `lm` call.

Another example is `hist`. Typically we don't want anything displayed in the console
when we call the function
```{r fig.keep="none"}
x = rnorm(x)
hist(x)
```
However if we assign the output to an object, `out = hist(x)`, the object `out` is
actually a list containing, _inter alia_ information on the mid-points, breaks and
counts.

## Code profiling

A circumstance that happens all too often, is that we simple want our code to run
faster. In some cases it's obvious where the bottle neck lies. Other times, we have to
rely on our intuition. One major drawback of relying on our intuition is that we could
be wrong and we end up pointlessly optimising the wrong piece of code. To make slow
code run faster, we first need to determine where the slow code lives.

The `Rprof` function is built-in tool for profiling the execution of R expressions. At
regular time intervals, the profiler stops the R interpreter, records the current
function call stack, and saves the information to a file. The results from `Rprof` are
stochastic. Each time we run a function R, the conditions have changed. Hence, each
time you profile your code, the result will be slightly different.

Unfortunately, `Rprof` is not user friendly. The **profvis** package provides an
interactive graphical interface for visualising data data from `Rprof`.


### Getting started with **profvis**

Currently the **profvis** package isn't on CRAN and so we need to install the
developmental version
```{r eval=FALSE}
devtools::install_github("rstudio/profvis")
```
As a simple example, we will use the `movies` data set, which contain information on
around 60,000 movies from the IMDB. First, we'll select movies that are classed as
comedies, then plot year vs movies rating, and draw a local polynomial regression line
to pick out the trend. The main function from the **profvis** package, is `profvis`
which profiles the code and creates an interactive HTML page. The first argument of
`profvis` is the R expression of interest.

```{r, eval=FALSE}
library("profvis")
profvis({
  data(movies, package="ggplot2movies")
  movies = movies[movies$Comedy==1, ]
  plot(movies$year, movies$rating)
  model = loess(rating ~ year, data=movies)
  j = order(movies$year)
  lines(movies$year[j], model$fitted[j])
})
```

The above code provides an interactive HTML page (figure XXX). On the left side is the
code and on the right is a flame graph (horizontal direction is time in milliseconds
and the vertical direction is the call stack).

```{r echo=FALSE, fig.cap="Output from profvis"}
knitr::include_graphics("images/profvis1.png")
```

The left hand panel tells the amount of time spent on each line of code. We see that
majority of time is spent calculating the `loess` smoothing line. The bottom line of
the right panel also highlights that most of the execution time is spent on the
`loess` function. Travelling up the function, we see that `loess` calls `simpleLoess`
which in turn calls `.C` function.

The conclusion from this graph, is that if optimisation were required, it would make
sense to focus on the `loess` and possibly the `order` functions calls.
 
<!-- http://rpubs.com/wch/123888 -->

```{r, eval=FALSE, echo=FALSE}
## Code for screen shot
library("profvis")
x = 
  profvis({
    data(movies, package="ggplot2movies")
    movies = movies[movies$Comedy==1, ]
    plot(movies$year, movies$rating)
    model = loess(rating ~ year, data=movies)
    j = order(movies$year)
    lines(movies$year[j], model$fitted[j])
})
print(x, viewer=TRUE, width="1300px", height="300px")
```

### Example: Monopoly Simulation

Monopoly is a board games that originated in the United States over 100 years ago. The
object of the game is to go round the board and purchase squares (properties). If
other players land on your properties they have to pay a tax. The player with the most
money at the end of the game, wins. To make things more interesting, there are Chance
and Community Chest squares. If you land on one of these squares, you draw card, which
may send to you to other parts of the board. The other special square, is Jail. One
way of entering Jail is to roll three successive doubles.

The **efficient** package contains a simulate monopoly function that attempts to find
the probability of landing on a certain square using a Monte-Carlo algorithm. The
entire code is around 100 lines long. In order for `profvis` to fully profile the
code, the **efficient** package needs to be installed from source

```{r eval=FALSE}
## args is also a valid argument for install.packages
devtools::install_github("csgillespie/efficient_pkg", args="--with-keep.source")
```

The function can then be profiled via
```{r eval=FALSE}
library("efficient")
profvis(SimulateMonopoly(10000))
```

to get

```{r echo=FALSE}
knitr::include_graphics("images/profvis_monopoly.png")
```

The output from `profvis` shows that the vast majority of time is spent in the
`move` function. In Monopoly rolling a double (a pair of 1's, 2's,
..., 6's) is special:

 * Roll two dice (`total1`): `total_score = total1`;
 * If you get a double, roll again (`total2`) and `total_score = total1 + total2`;
 * If you get a double, roll again (`total3`) and `total_score = total1 + total2 + total3`;
 * If roll three is a double, Go To Jail, otherwise move `total_score`.

The `move` function spends around 50% of the time is spent creating a data frame, 25%
time calculating row sums, and the remainder on a comparison operations. This piece of
code can be optimised fairly easily (will still retaining the same overall structure)
by incorporating the following improvements[^Solutions are available in the
**efficient** package vignette.]:

  * Use a matrix instead of a data frame;
  * Sample $6$ numbers at once, instead of two groups of three;
  * Switch from `apply` to `rowSums`;
  * Use `&&` in the `if` condition.

Implementing this features, results in the following code.

```{r}
move = function(current) {
  ## data.frame -> matrix
  rolls = matrix(sample(1:6, 6, replace=TRUE), ncol=2)
  Total = rowSums(rolls)	# apply -> rowSums	
  IsDouble = rolls[,1] == rolls[,2]
  ## & -> &&
  if(IsDouble[1] && IsDouble[2] && IsDouble[3]) {
    current = 11#Go To Jail
  } else if(IsDouble[1] && IsDouble[2]) {
    current = current + sum(Total[1:3])
  } else if(IsDouble[1]) {
    current = current + Total[1:2]
  } else {
    current = Total[1]
  }
  current
}
```

which gives a 25-fold speed improvement.

#### Exercise {-}

The `move` function uses a vectorised solution. Whenever we move, we always roll six
dice, then examine the outcome and determine the number of doubles. However, this is
potentially wasteful, since the probability of getting one double is $1/6$ and two
doubles is $1/36$. Another method is too only roll additional dice if and when they
are needed. Implement and time this solution.





## The byte compiler

The ** compiler** package, written by R Core member Luke Tierney has been part of R
since version 2.13.0. The **compiler** package allows R functions to be compiled,
resulting in a byte code version that may run faster^[The authors have yet to find a
situation where byte compiled code runs significantly slower.]. The compilation
process eliminates a number of costly operations the interpreter has to perform, such
as variable lookup.

Since R 2.14.0, all of the standard functions and packages in base R are pre-compiled
into byte-code. This is illustrated by the base function `mean`:

```{r}
mean
```

The third line contains the `bytecode` of the function. This means that the
**compiler** package has translated the R function into another language that can be
interpreted by a very fast interpreter. Amazingly the **compiler** package is almost
entirely pure R, with just a few C support routines.

### Example: the mean function

The **compiler** package comes with R, so we just need to load the package in the
usual way

```{r}
library("compiler")
```

Next we create an inefficient function for calculating the mean. This function takes
in a vector, calculates the length and then updates the `m` variable.

```{r}
mean_r = function(x) {
  m = 0
  n = length(x)
  for(i in seq_len(n))
    m = m + x[i]/n
  m
}
```

This is clearly a bad function and we should just `mean` function, but it's a useful
comparison. Compiling the function is straightforward

```{r}
cmp_mean_r = cmpfun(mean_r)
```

Then we use the `benchmark` function to compare the three variants


<!-- Make n bigger and just copy and paster output -->
```{r results="hide", cache=TRUE}
# Generate some data
x = rnorm(1000)
rbenchmark::benchmark(mean_r(x), cmp_mean_r(x), mean(x), 
          columns=c("test", "elapsed", "relative"),
          order="relative", replications=5000)
```

The compiled function is around seven times faster than the uncompiled function. Of
course, the native `mean` function is faster, but compiling does make a significant
difference (figure \@ref(fig:6-4)).

```{r 6-4, echo=FALSE, fig.height=4, fig.width=6, fig.cap="Comparsion of mean functions.", eval=TRUE}
local(source("code/10-performance_f1.R", local=TRUE))
```

### Compiling code

There are a number of ways to compile code. The easiest is to compile individual
function using `cmpfun`, but this obviously doesn't scale. If you create a package,
you automatically compile the package on installation by adding
```
ByteCompile: true
```
to the `DESCRIPTION` file. Most R packages installed using `install.packages` are not
compiled. We can enable (or force) packages to be compiled by starting R with the
environment variable `R_COMPILE_PKGS` set to a positive integer value.

A final option to use just-in-time (JIT) compilation. The `enableJIT` function
disables JIT compilation if the argument is `0`. Arguments `1`, `2`, or `3` implement
different levels of optimisation. JIT can also be enabled by setting the environment
variable `R_ENABLE_JIT`, to one of these values.

```{block, type="rmdtip"}
I always set the compile level to the maximum value of 3.
```

```{r eval=FALSE, echo=FALSE}
dd = NULL
for(i in seq(2, 4, length.out=12)) {
  x = rnorm(10^i)
  dd_tmp = rbenchmark::benchmark(my_mean(x), cmp_mean(x), mean(x), 
                                 columns=c("test", "elapsed", "relative"),
                                 order="relative", replications=5000)
  dd_tmp$i = i
  dd = rbind(dd, dd_tmp)
}
dd$p = 10^dd$i
dir.create("data", showWarnings = FALSE)
mean_comparison = dd
save(mean_comparison, file="data/mean_comparison.RData")
```








