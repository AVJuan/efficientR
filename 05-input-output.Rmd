---
knit: "bookdown::preview_chapter"
---

# Efficient input/output

Input/output (I/O) is the process of getting information into a particular computer system (in this case R) and then exporting it to the 'outside world' again (in this case as a file format that other software can read). Data I/O will be needed on projects where data comes from, or goes to, external sources. Yet the majority of R resources and documentation start with the assumption that your data has already been loaded. But importing datasets into R, and exporting them to the world outside the R ecosystem, can be a time-consuming and frustrating process. If the process is tricky, slow or ultimately unsuccessful, this can represent a major inefficiency right at the outset of a project. Conversely, 

This section explains how to efficiently read a wide range of datasets into R. We will look at getting data from the internet (this section is covered first because an increasing proportion of the world's data is available online); efficiently importing data from a wide variety of data formats with the **rio** package; accessing data stored in R packages; 

With the accelerating digital revolution, an . Further, an increasing proportion of data is *open access*, free for anyone to download. For these reasons downloading and importing data from the web is the first . Next we briefly outline two developments for data import: the **rio** package and the `.feather` data format, which can make data import efficient from programmer and computational perspectives. The section finishes with an exploration of how functions for reading in files stored in common *plain text* file formats from the **readr** and **data.table** packages can improve load speeds when working with these files.

Before reading in a single line of data, however, it is worth considering a general principle for reproducible data management: never modify raw data files. Raw data should be seen as read-only, and contain information about its provenance. Keeping the original file name and commenting on its origin are a couple of ways to improve reproducibility, even when the data are not publicly available.


## Top 5 tips for efficient data carpentry

- File format decisions can have a huge impact on the efficiency of data import and export operations. Use `.Rds` files with `readRDS` and `saveRDS` for keeping local copies of clean data.

## Getting data from the internet

The code chunk below shows how the functions
`download.file`^[Since
R 3.2.3 the base function `download.file()` can be used to download from secure (`https://`) connections on any operating system.
]
and `unzip` can be used to download and unzip a dataset from the internet.
R can automate processes that are often performed manually, e.g. through the graphical user interface of a web browser, with potential advantages for reproducibility and programmer efficiency. The result is data stored neatly in the `data` directory ready to be imported. Note we deliberately kept the file name intact help with documentation, enhancing understanding of the data's *provenance*. Note also that part of the dataset is stored in the **efficient** package.

```{block, dutch-data, type='rmdnote'}
Using R for basic file management can help create a reproducible workflow, as illustrated below. The data downloaded in the following code chunk is a multi-table dataset on Dutch naval expeditions used with permission from the CWI Database Architectures Group and described more fully at [monetdb.org](https://www.monetdb.org/Documentation/UserGuide/MonetDB-R). From this dataset we primarily use the 'voyages' table with lists Dutch shipping expeditions by their date of departure.
```

```{r, eval=FALSE}
url = "https://www.monetdb.org/sites/default/files/voc_tsvs.zip"
download.file(url, "voc_tsvs.zip") # download file
unzip("voc_tsvs.zip", exdir = "data") # unzip files
file.remove("voc_tsvs.zip") # tidy up by removing the zip file
```

This workflow equally applies to downloading and loading single files. Note that one could make the code more concise by entering replacing the second line with `df = read.csv(url)`. However, we recommend downloading the file to disk so that if for some reason it fails (e.g. if you would like to skip the first few lines), you don't have to keep downloading the file over and over again. The code below downloads and loads data on atmospheric concentrations of CO^2^. Note that this dataset is also available from the **datasets** package.

```{r, eval=FALSE}
url = "https://vincentarelbundock.github.io/Rdatasets/csv/datasets/co2.csv"
download.file(url, "data/co2.csv")
```

```{r}
df_co2 = read.csv("data/co2.csv")
```

There are now many R packages to assist with the download and import of data. The organisation [ROpenSci](https://ropensci.org/) supports a number of these.
The example below illustrates this using the WDI package (not supported by ROpenSci) to accesses World Bank data on CO2 emissions in the transport sector:

```{r, eval=FALSE}
library("WDI") # load the WDI library (must be installed)
WDIsearch("CO2") # search for data on a topic
co2_transport = WDI(indicator = "EN.CO2.TRAN.ZS") # import data
```

There will be situations where you cannot download the data directly or when the data cannot be made available. In this case, simply providing a comment relating to the data's origin (e.g. `# Downloaded from http://example.com`) before referring to the dataset can greatly improve the utility of the code to yourself and others. 

```{r, eval=FALSE, echo=FALSE}
# Not shown as a distraction (RL)
# download data from nasa, as described here:
# http://disc.sci.gsfc.nasa.gov/recipes/?q=recipes/How-to-Read-Data-in-netCDF-Format-with-R
library("raster") # requires the ncdf4 package to be installed
r = raster("data/nc_3B42.20060101.03.7A.HDF.Z.ncml.nc")
```

## Versatile data import with rio

**rio** is is a 'A Swiss-Army Knife for Data I/O', providing easy-to-use and highly performant wrapper functions for importing a range of file formats. At the time of writing, these include `.csv`, `.feather`, `.json`, `.dta`, `.xls`, `.xlsx` and Google Sheets (see the package's [github page](https://github.com/leeper/rio) for up-to-date information). Below we illustrate three of **rio**'s key functions.

```{r, eval=FALSE}
library(rio)

# Specify a file
fname = system.file("extdata/voc_voyages.tsv", package = "efficient")

# Import the file (uses the fread function from data.table)
voyages = import(fname)

# Export the file as an Excel spreadsheet
export(voyages, "output.xlsx")
```

```{block, json, type='rmdtip'}
The ability to import and use `.json` data is becoming increasingly common as it a standard output format for many APIs and the **jsonlite** and **geojsonio** packages have been developed to make this as easy as possible.
```

### Exercises {-}

The final line in the code chunk above shows a neat feature of **rio** and some other packages: the output format is determined by the suffix of the file-name, which make for concise code. Try opening the `output.xlsx` file with an editor such as LibreOffice Calc or Microsoft Excel to ensure that the export worked, before removing this rather inefficient and non-secure file format from your system to preserve precious disk space:

```{r}
file.remove("output.xlsx")
```

## Accessing data stored in packages

Most well documented packages provide some example data for you to play with. This can help demonstrate use cases in specific domains, that uses a particular data format. The command `data(package = "package_name")` will show the datasets in a package. Datasets provided by **dplyr**, for example, can be viewed with `data(package = "dplyr")`.

Raw data (i.e. data which has not been converted into R's native `.Rds` format) is usually located with the sub-folder `extdata` in R (which corresponds to `inst/extdata` when developing packages. The function `system.file` outputs file paths associated with specific packages. To see all of the external files within the **readr** package, for example, one could use the following command:

```{r}
list.files(system.file("extdata", package = "readr"))
```

Further, to 'look around' to see what files are stored in a particular package, one could type the following, taking advantage of RStudio's intellisense file completion capabilities (using copy and paste to enter the file path):

```{r, eval=FALSE}
system.file(package = "readr")
#> [1] "/home/robin/R/x86_64-pc-linux-gnu-library/3.3/readr"
"/home/robin/R/x86_64-pc-linux-gnu-library/3.3/readr/"
```

Hitting `Tab` after the second command should trigger RStudio to create a miniature pop-up box listing the files within the folder, as illustrated in figure \@ref(fig:intelli).

```{r intelli, echo=FALSE, fig.cap="Discovering files in R packages using RStudio's 'intellisense'.", out.width="50%", fig.align='center'}
knitr::include_graphics("figures/rstudio-package-filepath-intellisense.png")
```





## The feather file format

Feather was developed as collaboration between R and Python developers to create a fast, light and language agnostic format for storing data frames. The code chunk below shows how it can be used to save and then re-load the `df_co2` dataset loaded previously in both R and Python:

```{r, eval=FALSE}
library("feather")
write_feather(df_co2, "data/co2.feather")
df_co2_feather = read_feather("data/co2.feather")
```

```{r, engine='python', eval=FALSE}
import feather
import feather
path = 'data/co2.feather'
df_co2_feather = feather.read_dataframe(path)
```

## Efficient data export: .Rdata or .Rds?

Once you have tidied you data (described in the next Section), it will hopefully be suitably ship shape to save. Beyond the raw data, which should also be saved, saving it after tidying is recommended to reduce the chance of having to run all the data cleaning code again. However, it may also make sense to save your data in a new format early on, not least because read and write speeds of proprietary formats can be very slow. A large `.shp` file, for example, can take more than ten times longer to load than a `.Rds` or `.Rdata` file.

`.Rds` and `.RData` are R's native file format. This is a binary file format optimised for speed and compression ratios. But what is the difference between them? The follow code chunk demonstrates the key difference between these two (but surprisingly little known and used) file formats:

```{r}
save(df_co2, file = "data/co2.RData")
saveRDS(df_co2, "data/co2.Rds")
load("data/co2.RData")
df_co2_rds = readRDS("data/co2.Rds")
identical(df_co2, df_co2_rds)
```

The first method is the most widely used. It uses uses the `save` function which takes any number of R objects and writes them to a file, which must be specified by the `file =` argument. `save` is like `save.image`, which saves *all* the objects currently loaded in R.

The second method is slightly less used but we recommend it. Apart from being slightly more concise for saving single R objects, the `readRDS` function is more flexible: as shown in the subsequent line, the resulting object can be assigned to any name. In this case we called it `df_co2_rds` (which we show to be identical to `df_co2`, loaded with the `load` command) but we could have called it anything or simply printed it to the console.

Using `saveRDS` is good practice because it forces you to specify object names. If you use `save` without care, you could forget the names of the objects you saved and accidentally overwrite objects that already existed.

How space efficient are these file export methods? We can explore this question using the functions `list.files` and `file.size`, as illustrated below. The results, which also show how the *relative* space saving of native R formats increase with dataset size, are shown in Table \@ref(tab:sizes).

```{r}
files_co2 = list.files(path = "data", pattern = "co2.", full.names = TRUE)
filesize_co2 = data.frame(
  Format = gsub(pattern = "data/", replacement = "", files_co2),
  Size = file.size(files_co2) / 1000
  )
```

```{r spacebench, echo=FALSE, eval=FALSE}
filesize_co2$`Rel` = filesize_co2$Size / min(filesize_co2$Size)
filesize_co2$Size_10

for(i in 10^(1:3)){
  size_name = paste0("Size (", i, "x)")
  relsize_name = paste0("Rel (", i, "x)")
  df = do.call("rbind", replicate(i, df_co2, simplify = FALSE))
  write.csv(df, file.path(tempdir(), "co2.csv"))
  write_feather(df, file.path(tempdir(), "co2.feather"))
  save(df, file = file.path(tempdir(), "co2.RData"))
  saveRDS(df, file.path(tempdir(), "co2.Rds"))
  sizes = c(
    file.size( file.path(tempdir(), "co2.csv")),
    file.size( file.path(tempdir(), "co2.feather")),
    file.size( file.path(tempdir(), "co2.RData")),
    file.size( file.path(tempdir(), "co2.Rds"))
  )
  filesize_co2[[size_name]] = sizes / 1000
  filesize_co2[[relsize_name]] = sizes / min(sizes)
}
saveRDS(filesize_co2, "data/filesizes.Rds")
```

```{r sizes, echo=FALSE}
filesize_co2 = readRDS("data/filesizes.Rds")
knitr::kable(filesize_co2, digits = 1, row.names = FALSE, caption = "Absolute (MB) and relative (compared with the smallest size for each column) disk usage for the 3 column, 468 row 'co2' dataset, saved in different formats. Columns headed 10x, 100x and 1000x show the results for disk usage after increasing the number of rows by 10, 100 and 1000 fold respectively.")
```

The results of this simple disk usage benchmark show the advantages of saving data in a compressed binary format can be great, from hard-disk and, if your data will be shared on-line, data download time and bandwidth usage perspectives. It is striking to note that R's native formats can be **over 100 times** more space efficient than plain text (.csv) and other binary (`.feather`) formats. But how does each method compare from a computational efficiency perceptive?

## A benchmark of methods for file import and export

The read and write times for the functions showcased above are presented in Table \@ref(tab:rtimes) and Table \@ref(tab:wtimes) respectively.

```{r, echo=FALSE, eval=FALSE}
# Create data frame for reporting read/write times
wtimes = data.frame(
  Function = c("write.csv", "save_feather", "save", "saveRDS"))
rtimes = rtimes = data.frame(
  Function = c("read.csv", "read_feather", "load", "readRDS"))

# Benchmark read/write times
library("microbenchmark")

for(i in 10^(0:3)){
  time_name = paste0("Time (", i, "x)")
  reltime_name = paste0("Rel (", i, "x)")
  df = do.call("rbind", replicate(i, df_co2, simplify = FALSE))
  times_w = c(
  summary(microbenchmark(write.csv(df, file.path(tempdir(), "co2.csv")),
                         unit = "ms", times = 10))$mean,
  summary(microbenchmark(write_feather(df, file.path(tempdir(), "co2.feather")),
                         unit = "ms", times = 10))$mean,
  summary(microbenchmark(save(df, file = file.path(tempdir(), "co2.RData")),
                         unit = "ms", times = 10))$mean,
  summary(microbenchmark(saveRDS(df, file.path(tempdir(), "co2.Rds")),
                         unit = "ms", times = 10))$mean
  )
  times_r = c(
  summary(microbenchmark(read.csv(file.path(tempdir(), "co2.csv")),
                         unit = "ms", times = 10))$mean,
  summary(microbenchmark(read_feather(file.path(tempdir(), "co2.feather")),
                         unit = "ms", times = 10))$mean,
  summary(microbenchmark(load(file.path(tempdir(), "co2.RData")),
                         unit = "ms", times = 10))$mean,
  summary(microbenchmark(readRDS(file.path(tempdir(), "co2.Rds")),
                         unit = "ms", times = 10))$mean
)
  wtimes[[time_name]] = times_w 
  wtimes[[reltime_name]] = times_w / min(times_w)
  rtimes[[time_name]] = times_r 
  rtimes[[reltime_name]] = times_r / min(times_r)
}
names(rtimes) = names(wtimes) = gsub(" \\(1x\\)", "", names(wtimes))
saveRDS(rtimes, "data/rtimes.Rds")
saveRDS(wtimes, "data/wtimes.Rds")
```

```{r rtimes, echo=FALSE}
rtimes = readRDS("data/rtimes.Rds")
knitr::kable(rtimes, digits = 1, row.names = FALSE, caption = "Absolute and relative (compared with the smallest size for each column) read times for the 3 column, 468 row 'co2' dataset, saved with different functions. Columns headed 10x, 100x and 1000x show the results for disk usage after increasing the number of rows by 10, 100 and 1000 fold respectively.")
```

```{r wtimes, echo=FALSE}
wtimes = readRDS("data/wtimes.Rds")
knitr::kable(wtimes, digits = 1, row.names = FALSE, caption = "Absolute and relative (compared with the smallest size for each column) write times for the 3 column, 468 row 'co2' dataset, saved with different functions. Columns headed 10x, 100x and 1000x show the results for disk usage after increasing the number of rows by 10, 100 and 1000 fold respectively.")
```

The results show that the relative *size* of different formats is not a reliable predictor of data read and write times. This is due to the computational overheads of compression. Although the binary `.feather` format did not perform well in terms of read and write times, the function `read_feather` is *faster*  than R's native functions for saving `.Rds` and `.RData` formats, for the datasets used in the benchmark. `write_feather` is also faster than `save` and `saveRDS` for all but the largest dataset. In all cases, `read.csv` and `write.csv` is several times slower than the binary formats and this relative slowness worsens with increasing dataset size. In the next section we explore the performance of alternatives to these base R functions for reading and writing plain text data files.

## Fast import of plain text formats {#fread}

There is often more than one way to read data into R. A simple `.csv`, for example, file can be imported using a wide range of methods, with implications for computational efficiency. This section investigates methods for getting data into R, with a focus on delimited text formats, as these are ubiquitous, and a focus on three approaches: base R's plain text reading functions such as `read.delim`, which are derived from `read.table`; the **data.table** approach, which uses the function `fread`; and the newer **readr** package which provides `read_csv` and other `read_` functions such as `read_tsv`.

```{block, read-table-csv, type='rmdnote'}
Note that a function 'derived from' another in this context means that it calls another function. The functions such as `read.csv` and `read.delim` in fact are *wrappers* for the more generic function `read.table`. This can be seen in the source code of `read.csv`, for example, which shows that the function is roughly the equivalent of `read.table(file, header = TRUE, sep = ",")`.
```

Although this section is focussed on reading text files, it demonstrate the wider principle that the speed and flexibility advantages of additional read functions can be offset by the disadvantages of addition package dependency (in terms of complexity and maintaining the code) for small datasets. The real benefits kick in on large datasets. Of course, there are some data types that *require* a certain package to load in R: the **readstata13** package, for example, was developed solely to read in `.dta` files generated by versions of Stata 13 and above.

Figure \@ref(fig:readr-vs-base) demonstrates that the relative performance gains of the **data.table** and **readr** approaches increase with data size, especially so for data with many rows. Below around 1 MB `read.delim` is actually *faster* than `read_csv` while `fread` is much faster than both, although these savings are likely to be inconsequential for such small datasets.

For files beyond 100 MB in size `fread` and `read_csv` can be expected to be around *5 times faster* than `read.delim`. This efficiency gain may be inconsequential for a one-off file of 100 MB running on a fast computer (which still takes less than a minute with `read.csv`), but could represent an important speed-up if you frequently load large text files. 

```{r readr-vs-base, fig.cap="Benchmarks of `base`, `data.table` and `readr` functions for reading csv files. The facets ranging from 2 to 200 represent the number of columns in the csv file..", echo=FALSE, fig.height=4, fig.width=6,warning=FALSE, message=FALSE}
local(source("code/04-project-planning_f3.R", local=TRUE))
```

When tested on a large (4 GB) .csv file it was found that `fread` and `read_csv` were almost identical in load
times and that `read.csv` took around 5 times longer. This consumed more than 10 GB of RAM, making it unsuitable to
run on many computers (see Section \@ref(ram) for more on memory). Note that both **readr** and base methods can be
made significantly faster by pre-specifying the column types at the outset (see below). Further details are provided by the help in `?read.table`.

```{r eval=FALSE}
read.csv(file_name, colClasses = c("numeric", "numeric"))
```

```{r, eval=FALSE, echo=FALSE}
# This script illustrates the download and processing of a massive (~4 Gb) csv file
# Aim is to test read times on large datasets
url = "http://download.cms.gov/nppes/NPPES_Data_Dissemination_Aug_2015.zip"

# download a large dataset - don't run
download.file(url, "data/largefile.zip")
# unzip the compressed file, measure time
system.time( 
  unzip("data/largefile.zip", exdir = "data")
  )
##    user  system elapsed 
##  34.380  22.428 193.145

bigfile = "data/npidata_20050523-20150809.csv"
file.info(bigfile) # file info (not all shown)
##       size: 5647444347

system.time(df1 = read.csv(bigfile))
system.time(df2 = fread(bigfile))
system.time(df3 = read_csv(bigfile))
## Error (from 32 bit machine): cannot allocate vector of 32.0 Mb
```

<!-- Idea: test the functions on text data -->

In some cases with R programming there is a trade-off between speed and robustness. This is illustrated below with reference to differences in how **readr**, **data.table** and base R approaches handle unexpected values. Table \@ref(tab:voyages) shows that `read_tsv` is around 3 times faster, re-enforcing the point that the benefits of efficient functions increase with dataset size (made with Figure \@ref(fig:readr-vs-base)). This is a small (1 MB) dataset: the relative difference between `fread` and `read_` functions will tend to decrease as dataset size increases.

```{r, eval=FALSE}
library("microbenchmark")
library("readr")
library("data.table")
fname = system.file("extdata/voc_voyages.tsv", package = "efficient")
res_v = microbenchmark(times = 10,
  base_read = voyages_base <- read.delim(fname),
  readr_read = voyages_readr <- read_tsv(fname),
  dt_fread = voyages_dt <- fread(fname))
```

```{r, echo=FALSE}
# saveRDS(res_v, "data/res_v.Rds") # re-save if updated
res_v = readRDS("data/res_v.Rds")
```

```{r voyages-pre, echo=FALSE, results='hide', warning=FALSE}
# Running results here (not in benchmark) for fast build
library("microbenchmark")
library("readr")
library("data.table")
fname = system.file("extdata/voc_voyages.tsv", package = "efficient")
voyages_base = read.delim(fname)
voyages_readr = read_tsv(fname)
voyages_dt = fread(fname)
df = print(res_v)
df = dplyr::select(df, Function = expr, min, mean, max)
df[2:ncol(df)] = df[2:ncol(df)] / df$mean[3]
```

```{r voyages, echo=FALSE}
knitr::kable(df, caption = "Execution time of base, **readr** and **data.table** functions for reading in a 1 MB dataset relative to the mean execution time of `fread`, around 0.02 seconds on a modern computer.", digits = 1)
```

The benchmark above produces warning messages (not shown) for the `read_tsv` and `fread` functions but not the slowest base function `read.delim`. An exploration of these functions can shed light on the speed/robustness trade-off.

- The **readr** function `read_csv` generates a warning for row 2841 in the `built` variable.
This is because `read_*()` decides what class each variable is based on the first 1000 rows, rather than all rows, as base `read.*` functions do.

As illustrated by printing the result for the row which generated a warning, the `read_tsv` output is more sensible than the `read.delim` output: `read.delim` coerced the date field into a factor based on a single entry which is a text. `read_tsv` coerced the variable into a numeric vector, as illustrated below. 

```{r}
class(voyages_base$built) # coerced to a factor
class(voyages_readr$built) # numeric based on first 1000 rows
voyages_base$built[2841] # contains the text responsible for coercion
voyages_readr$built[2841] # an NA: text cannot be converted to numeric
```

- The **data.table** function `fread` generates 5 warning messages stating that columns 2, 4, 9, 10 and 11 were `Bumped to type character on data row ...`, with the offending rows printed in place of `...`. Instead of changing the offending values to `NA`, as **readr** does for the `built` column (9), `fread` automatically converts any columns it thought of as numeric into characters. An additional feature of `fread` is that it can read-in a selection of the columns, either by their index or name, using the `select` argument. This is illustrated below by reading in only half (the first 11) columns from the voyages dataset and comparing the result with `fread`'ing all the columns in.

```{r, warning=FALSE}
microbenchmark(times = 5,
  with_select = fread(fname, select = 1:11),
  without_select = fread(fname)
  )
```

To summarise, the differences between base, **readr** and **data.table** functions for reading in data go beyond code execution times. The functions `read_csv` and `fread` boost speed partially at the expense of robustness because they decide column classes based on a small sample of available data. The similarities and differences between the approaches are summarised for the Dutch shipping data (described in a note at the beginning of this section) in Table \@ref(tab:colclasses).

```{r colclasses, echo=FALSE}
vcols = as.data.frame(rbind(
  sapply(voyages_base, class),
  sapply(voyages_readr, class),
  sapply(voyages_dt, class)))
vcols = cbind(df[1], vcols)
vcols = dplyr::select(vcols, Function, number, boatname, built, departure_date)
knitr::kable(vcols, caption = "Execution time of base, **readr** and **data.table** functions for reading in a 1 MB dataset")
```

Table \@ref(tab:colclasses) shows 4 main similarities and differences between the three read types of read function:

- For uniform data such as the 'number' variable in Table \@ref(tab:colclasses), all reading methods yield the same result (integer in this case).
- For columns that are obviously characters such as 'boatname', the base method results in factors (unless `stringsAsFactors` is set to `TRUE`) whereas `fread` and `read_csv` functions return characters.
- For columns in which the first 1000 rows are of one type but which contain anomalies, such as 'built' and 'departure_data' in the shipping example, `fread` coerces the result to characters.
`read_csv` and siblings, by contrast, keep the class that is correct for the first 1000 rows and sets the anomalous records to `NA`. This is illustrated in \@ref(tab:colclasses), where `read_tsv` produces a `numeric` class for the 'built' variable, ignoring the non numeric text in row 2841.
- `read_*` functions generate objects of class `tbl_df`, an extension of the `data.frame`, as discussed in Section  \@ref(dplyr). `fread` generates objects of class `data.table`. These can be used as standard data frames but differ subtly in their behaviour.

The wider point associated with these tests is that functions that save time can also lead to additional considerations or complexities your workflow. Taking a look at what is going on 'under the hood' of fast functions to increase speed, as we have done in this section, can help understand the knock-on consequences of choosing fast functions over slower functions from base R. 

```{r, eval=FALSE, tidy=FALSE, echo=FALSE}
# # This was a final sentence from the previous paragraph that I've removed for now: 
# In some cases there will be no knock-on consequences of using faster functions provided by packages but you should be aware that it is a possibility. 
# I've removed this for now as it's such a large and unwieldy dataset
url = "http://download.cms.gov/nppes/NPPES_Data_Dissemination_Aug_2015.zip"
download.file(url, "largefile.zip") # takes many minutes
unzip("largefile.zip", exdir = "data") # many minutes
bigfile = "npidata_20050523-20150809.csv"
file.info(bigfile) # file info (4 GB+)
# split -b1000m npidata_20050523-20150809.csv # original command commented out
```

### Preprocessing outside R

There are circumstances when datasets become too large to read directly into R.
Reading in 4 GB text file using the functions tested above, for example, consumed all available RAM on an 16 GB machine!
To overcome the limitation that R reads all data directly into RAM, external *stream processing* tools can be used to preprocess large text files.
The following command, using the shell command `split`, for example, would break a large multi GB file many one GB chunks, each of which is more manageable for R:

```{r, engine='bash', eval=FALSE}
split -b100m bigfile.csv
```

The result is a series of files, set to 100 MB each with the `-b100m` argument in the above code. By default these will be called `xaa`, `xab` and which could be read in *one chunk at a time* (e.g. using `read.csv`, `fread` or `read_csv`, described in the previous section) without crashing most modern computers.

Splitting a large file into individual chunks may allow it to be read into R.
This is not an efficient way to import large datasets, however, because it results in a non-random sample of the data this way.
A more efficient way to work with very large datasets is via databases.
