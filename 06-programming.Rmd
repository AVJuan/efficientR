---
output: pdf_document
---
# Efficient programming

```{r echo=FALSE}
source("R/initialise.R")
```
<!--# Efficient programming  {#programming}-->

In this chapter we will discuss key R data types and idiomatic programming style. 
Many people that use R would not describe themselves as "programmers". Instead, they have
advanced domain level knowledge, but little formal training in programming. 
This chapter comes from their point of view; someone who has use standard R data structures, such
as vectors and data frames, but has never looked as the inner workings of these objects.

We begin this chapter by discussing key data types, how they are used and potential computational 
gains available. Once we understand these objects, we will look at key R programming idioms, before 
covering techniques for speeding up code.

## Data types

A data type is an object that has a set of predifined characteristics, such as a number or a character.  When programming in C or FORTRAN, the data type of every object must be specified by the user. he advantage is that it allows the compiler to perform type-specific optimisation. 
The downside is verbose and fragile code, which is inefficient to type. 
In R data types are less critical, but understanding them will help you debug and optimize for computational efficiency. Essentially, we have a trade-off between CPU run time and developer thinking time.
However an understanding of data types can help when debugging and optimizing for computational efficiency. 
In this chapter, we will pick out the key point data types from an efficiency perspective.
Chapter 2 of Advanced R Programming [@Wickham2014] provides a more comprehensive treatment.

### Vectors

The vector is a fundamental data structure in R. Confusingly there are two varieties:

  * Atomic vectors are where all elements have the same type and are usually created using the `c()` function;
  * Lists are where elements can have different types.
  
To test if an object is a vector, we must use `is.atomic(x) || is.list(x)`. The more obvious choice for determining if an object is a vector, `is.vector(x)`, only returns `TRUE` is an object is a vector with no attributes other than names. For example, when we use the `table` function

```{r}
x = table(rpois(100, 5))
```

the object `x` has additional attributes (such as `dim`), so `is.vector(x)` return `FALSE`. But the contents `x` is clearly a vector, so `is.atomic(x)` returns `TRUE`.

The core vector data types are logicals, integers, doubles and characters. When an atomic vector is created with a mixture of types, the output type is coerced to highest type in the following hierarchy:

```
logical < integer < double < character 
```

This means that any vector containing a character string will be coerced to class, as illustrated below.

#### Numerics: doubles and integers

Numbers in R are usually stored in [double-precision floating-point format](https://goo.gl/ZA5R8a) - see @Braun2007 and @Goldberg1991. The term 'double' refers to the fact that on $32$ bit systems (for which the format was developed) two memory locations are used to store a single number. Each double-precision number occupies $8$ bytes and is accurate to around $17$ decimal places (R does not print all of these, as you will see by typing `pi`). Somewhat surprisingly, when we run the command

```{r}
x = 1
```

we have created an atomic vector, contain a single double-precision floating point number. 
When comparing floating point numbers, we should be particularly careful, since

```{r}
y = sqrt(2)*sqrt(2)
y == 2
```

This is because the value of `y` is not exactly $2$, instead it's __almost__ $2$

```{r}
sprintf("%.16f", y)
```

To compare numbers in R it is advisable to use `all.equal` and set an appropriate tolerance, e.g.

```{r}
all.equal(y, 2, tolerance = 1e-9)
```

Although using double precision objects is the most common type, R does have other ways of storing numbers:

* `single`: R doesn't have a single precision data type. Instead, all real numbers are stored in double precision format. The functions `as.single` and `single` are identical to `as.double` and `double` except they set the attribute `Csingle` that is used in the `.C` and `.Fortran` interface.

* `integer`: Integers primarily exist to be passed to C or Fortran code. Typically we don't worry about creating integers. However they are occasionally used to optimise sub-setting operations. When we subset a data frame or matrix, we are interacting with C code. For example, if we look at the arguments for the `head` function

```{r}
args(head.matrix)
```

The default argument is `6L` (the `L`, is short for Literal and is used to create an integer). Since this function is being called by almost everyone that uses R, this low level optimisation is useful. To illustrate the speed increase, suppose we are selecting the first $100$ rows from a data frame (`clock_speed`, from the `efficient` package). The speed increase is illustrated below, using the `microbenchmark` package:

```{r, matrix_timing, eval=FALSE}
s_int = 1:100; s = seq(1, 100, 1.0)
microbenchmark(clock_speed[s_int, 2L], clock_speed[s, 2.0], times=1000000L)
```

```
## Unit: microseconds
## expr   min    lq  mean median    uq   max neval cld
## clock_speed[s_int, 2L] 11.79 13.43 15.30  13.81 14.22 87979 1e+06  a 
## clock_speed[s, 2] 12.79 14.37 16.04  14.76 15.18 21964 1e+06   b
```

The above result shows that using integers is slightly faster, but probably not worth worrying about.

* `numeric`: The function `numeric()` is identical to `double()`; it creates is a double-precision number. However, `is.numeric()` isn't the same as `as.double()`, instead `is.numeric()` returns `TRUE` for both numeric and double types.

To find out the type of data stored in an R vector use the command `typeof()`:

```{r}
typeof(c("a", "b"))
```

#### Exercises

A good way of determining how to use more advanced programming concepts, is to examine the source code of R.

1. What are the data types of `c(1, 2, 3)` and `1:3`?
1. Have a look at the following function definitions:
    * `tail.matrix`
    * `lm`
2. How does the function `seq.int`, which was used in the `tail.matrix` function, differ to the standard `seq` function? 

### Factors

A factor is useful when you know all of the possible values a variable may take. For example, suppose our data set related to months of the year

```{r}
m = c("January", "December", "March")
```

If we sort `m` in the usual way `sort(m)`, we use standard alpha-numeric ordering, placing December first. While this is completely correct, it is also not that helpful. We can use factors to remedy this problem by specifying the admissible levels

```{r}
## month.name contains the 12 months
fac_m = factor(m, levels=month.name)
sort(fac_m)
```

Most users interact with factors via the `read.csv` function where character columns are automatically converted to factors. It is generally recommended to avoid this feature using the `stringsAsFactors=FALSE` argument. Although this argument can be also placed in the global `options()` list, this leads to non-portable code, so should be avoided.

Although factors look similar to character vectors, they are actually integers. This leads to initially surprising behaviour

```{r}
c(m)
c(fac_m)
```

In this case the `c()` function is using the underlying integer representation of the factor. Overall factors are useful, but can lead to unwanted side-effects if we are not careful.

In early versions of R, storing character data as a factor was more space efficient. However since identical character strings now share storage, the space gain in factors is now space.


### Data frames

A data frame is a tabular (two dimensional or 'rectangular') object in which the columns may be composed of differing vector types such as `numeric`, `logical`, `character` and so on. 
Matrices can only accept a single data type for all cells as explained in the next section.
Data frames are the workhorses of R. Many R functions, such as `boxplot`, `lm` and `ggplot`, expect your data set to be in a data frame. As a general rule, columns in your data should be variables and rows should be the thing of interest. This is illustrated in the `USAarrests` data set:

```{r}
head(USArrests, 2)
```

Note that each row corresponds to a particular state and each column to a variable. One particular trap to be wary of is when using `read.csv` and `read.table` characters are automatically converted to factors. One can avoid this pitfall by using the argument `stringsAsFactors = FALSE`.

Since working with R frequently involves interacting with data frames, it's useful to be fluent a few key functions:

Table: Useful data frame functions.

Name | Description
-----|-----------
`dim`  | Data frame dimensions
`ncol`/`nrow` | No. of columns/rows
`NCOL`/`NROW` | As above, but also works with vectors
`cbind`/`rbind`| Column/row bind
`head`/`tail` | Select the first/last few rows
`colnames`/`rownames` | Column and row

When loading a dataset called `df` into R, a typical workflow would be:

* Check dimensions using `dim(df)`;
* Look at the first/last few rows using `head(df)` and `tail(df)`;
* Rename columns using `colnames(df) =`.

### Matrix

A matrix is similar to a data frame: it is a two dimensional object and sub-setting and other functions work in the same way. However all matrix columns must have the same type. 
Matrices tend to be used during statistical calculations. Linear regression using `lm()`, for example, internally converts the data to a matrix before calculating the results; any characters are thus recoded as numeric dummy variables.

Matrices are generally faster than data frames. The datasets `ex_mat` and `ex_df` from the efficient package each have $1000$ rows and $100$ columns. They contain the same random numbers. However, selecting rows from a data frame is around $150$ times slower than a matrix. This illustrates the reason for using matrices instead of data frames for efficient modelling in R:

```{r mat_vs_df, echo=2:4, cache=TRUE}
library("rbenchmark")
data(ex_mat, ex_df, package="efficient")
benchmark(replications=10000, 
          ex_mat[1,], ex_df[1,], 
          columns=c("test", "elapsed", "relative"))
```

### S3 objects {#S3}

R has three built-in object oriented systems. These systems differ in how classes and methods are defined. 
The easiest and oldest system is the S3 system. S3 refers to the third version of S. The
syntax of R is largely based on this version of S. In R there has never been S1 and S2 classes.

The S3 system implements a generic-function object oriented (OO) system. 
This type of OO is different to the message-passing style of Java and C++. 
In a message-passing framework, messages/methods are sent to objects and the
object determines which function to call, e.g. `normal.rand(1)`. 
The S3 class system is different. 
In S3, the generic function decides which method to call - it would have the form `rand(normal, 1)`.

The S3 system is based on the class of an object. In this system, a class is just an attribute. The S3 class(es) of a object can be determined with the `class` function.

```{r echo=2}
class(USArrests)
```

The S3 system can be used to great effect. For example, a `data.frame` is simply a standard R list, with class `data.frame`. When we pass an object to a _generic_ function, the function first examines the class of the object, and then decides what to do: it dispatches to another method. The generic `summary` function, for example, contains the following:

```{r}
summary
```

Note that the only operational line is `UseMethod("summary")`. This handles the method dispatch based on the object's class. So when `summary(USArrests)` is executed, the generic `summary` function passes `USArrests` to the function `summary.data.frame`. 

This simple mechanism enables us to quickly create our own functions.
Consider the distance object:

```{r}
dist_usa = dist(USArrests)
```

`dist_usa` has class `dist`. To visualise the distances, we can create an image method. First we'll check if the existing `image` function is generic, via

```{r}
image
```

Since `image` is already a generic method, we just have to create a specific `dist` method

```{r image_dist_s3}
image.dist = function(x, ...) {
  x_mat = as.matrix(x)
  image(x_mat, main=attr(x, "method"), ...)  
}
```

The `...` argument allows us to pass arguments to the main image method, such as `axes`:

```{r echo=2}
par(mar=c(1,1,2,1), mgp=c(0,0,0))
image(dist(USArrests), axes=FALSE)
```

Many S3 methods work in the same way as the simple `image.dist` function created above: the object is converted into a standard format, then passed to the standard method. Creating S3 methods for standard functions such as `summary`, `mean`, and `plot` provides a nice uniform interface to a wide variety of data types.

#### Exercises

1. Use a combination of `unclass` and `str` on a data frame to confirm that it is a list.
2. Use the function `length` on a data frame. What is return? Why?

### Efficient data structures

Even when our data set is small, the analysis can generate large objects. For example suppose we want to perform standard cluster analysis. Using the built-in data set `USAarrests`, we calculate a distance matrix:

```{r}
dist_usa = dist(USArrests)
```

The resulting object `dist_usa` measures the similarity between two states with respect to the input data. Since there are $50$ states in the `USAarrests` data set, this results in a matrix with $50$ columns and $50$ rows. Intuitively, since the matrix `dist_usa` is symmetric around the diagonal, it makes sense to exploit this characteristic for efficiency, allowing storage to be halved. If we examine the object `dist_usa`, with `str(dist_usa)`, it becomes apparent that the data is efficiently stored as a vector with some attributes.

Another efficient data structure is a sparse matrix. This is simply a matrix in where most of the elements are zero. Conversely, if most elements are non-zero, the matrix is considered dense. The proportion of non-zero elements is called the sparsity. Large sparse matrices often crop up when performing numerical calculations. Typically, our data isn't sparse but the resulting data structures we create may be sparse. There are a number of techniques/methods used to store sparse matrices. Methods for creating sparse matrices can be found in the `Matrix` package. For this `dist` object, since the structure is regular.

## Good programming techniques

A major benefit of using R (as opposed to C or Fortran, say), is that coding time is greatly reduced. 
However if we are not careful, it's very easy to write programs that are incredibly slow. 
While optimisations such as going parallel can easily double speed, poor code can easily run 100s of times slower. For this reason a priority of an efficient programmer should be to avoid the following common mistakes. If you spend any time programming in R, then reading [@Burns2011] should be considered essential reading.

### General tips

The key to making R code run fast is to access the underlying C/Fortran routines as quickly as possible. For example, suppose that `x` is a standard R vector of length `n`. Then 
```{r echo=3}
n = 10
x = runif(n)
x = x + 1
```
involves a single function call to the `+` function. Whereas,
```{r bad_loop}
for(i in 1:n) {
  x[i] = x[i] + 1 
}
```
has

  * `n` function calls to `+`;
  * `n` function calls to the `[` function;
  * `n` function calls to the `[<-` function (used in the assignment operation);
  *  A function call to `for` and the `:` operator. 

It isn't that the `for` loop is slow, rather it is because we calling many more functions. This point is indirectly tackled again in the section on vectorised code.

Another general technique is to be careful with memory allocation. In fact this could be considered the number $1$ rule when programming in R. If possible always pre-allocate your vector or data frame then fill in the values. Let's consider three methods of creating a sequence of numbers. 

__Method 1__ creates an empty vector, and grows the object

```{r echo=TRUE, tidy=FALSE}
method1 = function(n) {
  myvec = NULL
  for(i in 1:n)
    myvec = c(myvec, i)
  myvec
}
```

__Method 2__ creates an object of the final length and then changes the values in the object by subscripting:

```{r echo=TRUE, tidy=FALSE}
method2 = function(n) {
  myvec = numeric(n)
  for(i in 1:n)
    myvec[i] = i
  myvec
}
```

__Method 3__ directly creates the final object
```{r eval=TRUE, echo=TRUE}
method3 = function(n) 1:n
```

To compare the three methods we use the `benchmark` function from the previous chapter

```{r tidy=FALSE,cache=TRUE}
n = 1e4
benchmark(replications=10, 
          method1(n), method2(n), method3(n),
          columns=c("test", "elapsed"))
```

The table below shows the timing in seconds on my machine for these three methods for a
selection of values of $n$. The relationships for varying $n$ are all roughly linear on a log-log scale, but the timings between methods are drastically different. Notice that the timings are no longer trivial. When $n=10^7$, method 1 takes around an hour whilst method 2 takes $2$ seconds and method 3 is almost instantaneous.

$n$ | Method 1 | Method 2 | Method 3 
----|----------|----------|---------
$10^5$ | $\phantom{000}0.208$ | $0.024$ | $0.000$
$10^6$ | $\phantom{00}25.500$  | $0.220$ | $0.000$
$10^7$ | $3827.0000$             | $2.212$ | $0.000$

Table: Time in seconds to create sequences. When $n=10^7$, method 1 takes around an hour while methods 2 takes 2 seconds and method 3 almost instantaneous. 


### Caching variables

A straightforward method for speeding up code is to calculate objects once and reuse the value when necessary. This could be as simple with replacing `log(x)` in multiple function calls with the object `log_x` that is defined once and reused. This small saving in time, quickly multiplies when the cached variable is used inside a `for` loop. 

A more advanced form of caching is use the `memoise` package.
If a function is called multiple times with the same input, it may be possible to speed things up by keeping a cache of known answers that it can retrieve. The `memoise` package allows us easily store the value of function call and returns the cached result when the function is called again with the same arguments. This package trades off memory versus speed, since the memoised function stores all previous inputs and outputs. To cache a function, we simply pass the function to the `memoise` function.

```{r ch6_load_memoise, echo=FALSE, message=FALSE}
library("memoise")
library("rbenchmark")
```

The classic memoise example is the factorial function. Another example is to limit use to a web resource. For example, suppose we are developing a shiny (an interactive graphic) application where the user can fit regression line to data. The user can remove points and refit the line. An example function would be

```{r}
## Argument indicates row to remove
plot_mpg = function(row_to_remove) {
  data(mpg, package="ggplot2")
  mpg = mpg[-row_to_remove,]
  plot(mpg$cty, mpg$hwy)
  lines(lowess(mpg$cty, mpg$hwy), col=2)
}
```

We can use  `memoise` speed up by caching results. A quick benchmark

```{r benchmark_memoise, fig.keep="none", cache=TRUE, results="hide"}
m_plot = memoise(plot_mpg)
benchmark(m_plot(10), plot_mpg(10), columns = c("test", "relative", "elapsed"))
#         test relative elapsed
#1   m_plot(10)    1.000   0.007
#2 plot_mpg(10)  481.857   3.373
```

suggests that we can obtain a 500-fold speed-up.

### Function closures

More advanced caching is available using _function closures_. A closure in R is an object that contains functions bound to the environment the closure was created in. Technicially all functions in R have this property, but we use the term function closure to denote functions where the environment is not `.GlobalEnv`. One of the environments associated with function is known as the enclosing environment, that is, where was the function created. We can determine the enclosing environment using the `environment` function

```{r}
environment(plot_mpg)
```

The `plot_mpg` function's enclosing environment is the `.GlobalEnv`. This is important for variable scope, i.e. where should be look for a particular object. Consider the function `f`

```{r}
f = function() {
  x = 5
  function() {
    x
  }
}
```

When we call the function `f`, the object returned is a function. While the enclosing environment of `f` is `.GlobalEnv`, the enclosing environment of the __returned__ function is something different

```{r}
g = f()
environment(g)
```

When we call this new function `g`, 

```{r}
x = 10
g()
```

The value returned is obtained from `environment(g)` is `r environment(g)`, not `.GlobalEnv`. This environment allows to cache variables between function calls. The `counter` function is basic example of this feature

```{r}
counter = function() {
  no = 0
  count = function() {
    no <<- no + 1
    no
  }
}
```

When we call the function, we retain object values between function calls

```{r}
sc = counter()
sc()
sc()
```

The key points of the `counter` function are 

 * The counter function returns a function
    ```{r}
    sc = counter()
    sc()
    ```
  * The enclosing environment of `sc` is not `.GlobalEnv` instead, it's the binding environment of `sc`.
  * The function `sc` has an environment that can be used to store/cache values
  * The operator `<<-` is used to alter the `no`.

We can exploit function closures to simplify our code. Suppose we wished to simulate a games of Snakes and Ladders. We could have function that checked if we landed on a Snake, and if so move

```{r}
check_snake = function(square) {
   switch(as.character(square), 
       '16'=6,  '49'=12, '47'=26, '56'=48, '62'=19, 
       '64'=60, '87'=24, '93'=73, '96'=76, '98'=78, 
       square)
}
```
If we then wanted to determine how often we landed on a Snake, we could use a function closure to keep track 

```{r}
check_snake = function() {
  no_of_snakes = 0
  function(square) {
    new_square = switch(as.character(square), 
       '16'=6,  '49'=12, '47'=26, '56'=48, '62'=19, 
       '64'=60, '87'=24, '93'=73, '96'=76, '98'=78, 
       square)
    no_of_snakes = no_of_snakes + (new_square != square)
    new_square
  }
}
```

By keeping the variable `no_of_snakes` attached to the `check_snake` function, enables us to have cleaner code. 

### Vectorised code

When writing code in R, you need to remember that you are using R and not C (or even Fortran 77!). For example,

```{r eval=FALSE, echo=TRUE, tidy=FALSE}
## Change 1000 uniform random numbers
x = runif(1000) + 1
logsum = 0
for(i in 1:length(x))
  logsum = logsum + log(x[i])
```

is a piece R code that has a strong, unhealthy influence from C. Instead we should write

```{r eval=TRUE, echo=2}
x = 1
logsum = sum(log(x))
```

Writing code this way has a number of benefits.

* It's faster. When $n = 10^7$ the ``R way'' is about forty times faster.
* It's neater.
* It doesn't contain a bug when `x` is of length $0$.

Another common example is sub-setting a vector. When writing in C, we would have something like:

```{r tidy=FALSE}
ans = NULL
for(i in 1:length(x)) {
  if(x[i] < 0) 
    ans = c(ans, x[i])
}
```

This of course can be done simply with

```{r}
ans = x[x < 0]
```

```{r echo=FALSE,fig.width=4, fig.height=4}
set.seed(1)
par(mar=c(3,3,2,1), mgp=c(2,0.4,0), tck=-.01,
    cex.axis=0.9, las=1)
curve(x^2, 0,1, ylab="f(x)", xlab="x")
grid()
N = 40
px = runif(N); py=runif(N)
points(px[py < px^2], py[py < px^2], pch=19, col=1)
points(px[py > px^2], py[py > px^2], pch=19, col=2)
```

Figure 6.2 Example of Monte-Carlo integration. To estimate the area under the curve throw random points at the graph and count the number of points that lie under the curve.

#### Example: Monte-Carlo integration

It's also important to make full use of R functions that use vectors. For
example, suppose we wish to estimate
\[
\int_0^1 x^2 dx
\]
using a basic Monte-Carlo method. Essentially, we throw darts at the curve and count the number of darts
that fall below the curve (as in figure 6.2).

_Monte Carlo Integration_

1. Initialise: `hits = 0`
1. __for i in 1:N__
1. $~~~$ Generate two random numbers, $U_1, U_2$,  between 0 and 1
1. $~~~$ If $U_2 < U_1^2$, then `hits = hits + 1`
1. __end for__
1. Area estimate = `hits/N`

A standard C approach to implementing this Monte-Carlo algorithm would be something like:

```{r tidy=FALSE}
N = 500000
f = function(N){
  hits = 0
  for(i in 1:N)  {
    u1 = runif(1); u2 = runif(1)
    if(u1^2 > u2)
      hits = hits + 1
  }
  return(hits/N)
}
```

In R this takes a few seconds:

```{r cache=TRUE}
system.time(f(N))
```

In contrast, a more R-centric approach would be the following:

```{r echo=TRUE}
f1 = function(N){
  hits = sum(runif(N)^2 > runif(N))
  return(hits/N)
}
```

`f1` is around $30$ times faster than `f`, illustrating the efficiency gains that can be made by vectorising your code: 

```{r}
system.time(f1(N))
```

## Parallel computing

In recent R versions (since R 2.14.0) `parallel` package comes pre-installed with base R. The `parallel` package must still be loaded before use however, and you must determine the number of available cores manually, as illustrated below.

```{r echo=1:2}
library("parallel")
no_of_cores = detectCores()
```

The computer used to compile the published version of this book chapter has `r no_of_cores` CPUs/Cores. 

### Parallel versions of apply functions

The most commonly used parallel applications are parallelized replacements of `lapply`, `sapply` and `apply`. The parallel implementations and their arguments are shown below.

```{r eval=FALSE, tidy=FALSE}
parLapply(cl, x, FUN, ...)
parApply(cl = NULL, X, MARGIN, FUN, ...)
parSapply(cl = NULL, X, FUN, ..., simplify = TRUE, USE.NAMES = TRUE) 
```

Note that each function has an argument `cl` which must be created by `makeCluster`.
This function, amongst other things, specifies the number of processors to use. 

### Example: parallel bootstraping

In 1965, Gordon Moore co-founder of Intel, observed that the number of transistors in a dense integrated circuit doubles approximately every two years. This observation is known as Moore's law. A scatter plot (figure 6.3) of processors over the last thirty years shows that that this law seems to hold

```{r echo=FALSE, fig.width=4, fig.height=4, fig.cap="Transistor counts against introduction date. Credit: https://en.wikipedia.org/wiki/Transistor_count"}
par(mar=c(3,3,2,1), mgp=c(2,0.4,0), tck=-.01, cex.axis=0.9, las=1)
data("transistors", package="efficient")
plot(transistors$Year, log2(transistors$Count), 
     ylim=c(10, 35), xlim=c(1970, 2015), 
     ylab="Transistor Count (log2)", xlab="Year", 
     panel.first=grid(), pch=21, bg=3)
abline((lm(log2(Count) ~ Year, data=transistors)), col=4, lty=3, lwd=2)
```

Figure 6.3: Transistor counts against introduction date. Credit: https://en.wikipedia.org/wiki/Transistor_count

We can estimate the trend using simple linear regression. A standard algorithm for obtaining uncertainty estimates on regression coefficients is bootstrapping. This is a simple algorithm; at each iteration we sample with replacement from the original data set and estimate the parameters of the new data set. The distribution of the parameters gives us our uncertainty estimate. We begin by loading the data set and creating a function for performing a single bootstrap

```{r}
data("transistors", package="efficient")
bs = function(i) {
  s = sample(1:NROW(transistors), replace=TRUE)
  trans_samp = transistors[s,]
  coef(lm(log2(Count) ~ Year, data=trans_samp))
}
```

We can then perform $N=10^4$ bootstraps using `sapply`

```{r eval=FALSE}
N = 10000
sapply(1:N, bs)
```

Rewriting this code to make use of the `parallel` package is straightforward. We begin by making a cluster and exporting the data set

```{r, eval=FALSE}
library("parallel")
cl = makeCluster(6)
clusterExport(cl, "transistors")
```

Then use `parSapply` and stop the cluster

```{r eval=FALSE}
parSapply(cl, 1:N, bs)
stopCluster(cl)
```

On this computer, we get a four-fold speed-up.

```{r eval=FALSE}
stopCluster(cl)
```

### Process forking

Another way of running code in parallel is to use the `mclapply` and `mcmapply` functions. These functions use forking
forking, that is creating a new copy of a process running on the CPU. However, Windows does not support this low-level functionality in the way that Linux does. 

## The byte compiler

The compiler package, written by R Core member Luke Tierney has been part of R since version 2.13.0. Since R 2.14.0, all of the standard functions and packages in base R are pre-compiled into byte-code. This is illustrated by the base function `mean`:

```{r}
mean
```

The third line contains the `bytecode` of the function. This means that the compiler package translates the R function into another language that can be interpreted by a very fast interpreter.  

The `compiler` package allows R functions to be compiled, resulting in a byte code version that may run faster^[The authors have yet to find a situation where byte compiled code runs significantly slower.]. The compilation process eliminates a number of costly operations the interpreter has to perform, such as variable lookup. Amazingly the compiler package is almost entirely pure R, with just a few C support routines. 

### Example: the mean function

The compiler package comes with R, so we just need to load the package in the usual way

```{r}
library("compiler")
```

Next we create an inefficient function for calculating the mean. This function takes in a vector, calculates the length and then updates the `total` variable.

```{r}
my_mean = function(x) {
  total = 0
  n = length(x)
  for(i in 1:n)
    total = total + x[i]/n
  total
}
```

This is clearly a bad function and we should just `mean` function, but it's a useful comparison. Compiling the function is straightforward

```{r}
cmp_mean = cmpfun(my_mean)
```

Then we use the `benchmark` function to compare the three variants

```{r results="hide", cache=TRUE}
## Generate some data
x = rnorm(100)
benchmark(my_mean(x), cmp_mean(x), mean(x), 
          columns=c("test", "elapsed", "relative"),
          order="relative", replications=5000)
```

The compiled function is around seven times faster than the uncompiled function. Of course, the native `mean` function is faster, but the compiling does make a significant difference.

```{r echo=FALSE, fig.height=4, fig.width=4, fig.cap="Comparsion of mean functions.", eval=TRUE}
dd = readRDS(file="data/mean_comparison.RData")
library("ggplot2")



g = ggplot(dd, aes(p, relative)) + 
  geom_line(aes(colour=test), lwd=1) + 
  theme_bw() + 
  xlab("Sample size") + 
  ylab("Relative timings") + 
  scale_x_continuous(trans="log10", breaks=c(100, 1000, 10000), 
                     labels=c(expression(10^2), expression(10^3), expression(10^4))) + 
  scale_colour_manual(values=colours, guide=FALSE) +
  ylim(c(0, 200))
g + annotate("text", x=1000, y=90, label="Pure R", col=colours[3], size=3) +
  annotate("text", x=1000, y=20, label="Complied R", col=colours[1], size=3) + 
  annotate("text", x=8000, y=10, label="mean", col=colours[2], size=3)
```

Figure 6.4: Comparison of mean functions.

### Compiling code

There are a number of ways to compile code. The easiest is to compile individual function using `cmpfun`, but this obviously doesn't scale. If you create a package, then you automatically compile the package on installation by adding
```
ByteCompile: true
```
to the `DESCRIPTION` file. Most R packages installed using `install.packages` are not compiled. We can enable (or force) packages to be compiled by starting R with the environment variable `R_COMPILE_PKGS` set to a positive integer value.

A final option to use just-in-time (JIT) compilation. The `enableJIT` function disables JIT compilation if the argument is `0`. Arguments `1`, `2`, or `3` implement different levels of optimisation. JIT can also be enabled by setting the environment variable `R_ENABLE_JIT`, to one of these values.


```{r eval=FALSE, echo=FALSE}
dd = NULL
for(i in seq(2, 4, length.out=12)) {
  x = rnorm(10^i)
  dd_tmp = rbenchmark::benchmark(my_mean(x), cmp_mean(x), mean(x), 
                                 columns=c("test", "elapsed", "relative"),
                                 order="relative", replications=5000)
  dd_tmp$i = i
  dd = rbind(dd, dd_tmp)
}
dd$p = 10^dd$i
dir.create("data", showWarnings = FALSE)
saveRDS(dd, file="data/mean_comparison.RData")
```
